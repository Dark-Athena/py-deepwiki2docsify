#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepWiki to Docsify Converter 
ä¸“é—¨å¤„ç† DeepWiki ç”Ÿæˆçš„åœ¨çº¿é¡µé¢ï¼Œè½¬æ¢ä¸º Docsify é¡¹ç›®
Copyright (C) <2025-2028>  <darkathena@qq.com>
"""

import os
import re
import sys
import json
import time
import click
import requests
from pathlib import Path
from urllib.parse import urljoin, urlparse, unquote
from bs4 import BeautifulSoup
from markdownify import markdownify as md
import logging

# å°è¯•å¯¼å…¥ Selenium
try:
    from selenium import webdriver
    from selenium.webdriver.edge.service import Service as EdgeService
    from selenium.webdriver.edge.options import Options as EdgeOptions
    from selenium.webdriver.chrome.service import Service as ChromeService
    from selenium.webdriver.chrome.options import Options as ChromeOptions
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from webdriver_manager.microsoft import EdgeChromiumDriverManager
    from webdriver_manager.chrome import ChromeDriverManager
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class DeepWikiToDocsifyConverter:
    """DeepWiki åˆ° Docsify è½¬æ¢å™¨"""
    
    def __init__(self, base_url: str, output_dir: str = "./docs", use_selenium: bool = True, multilingual: bool = False, force_overwrite: bool = False):
        self.base_url = base_url.rstrip('/')
        self.output_dir = Path(output_dir)
        self.use_selenium = use_selenium and SELENIUM_AVAILABLE
        self.multilingual = multilingual
        self.force_overwrite = force_overwrite
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
        })
        self.driver = None
        
        # æ£€æŸ¥è¾“å‡ºç›®å½•
        self._check_output_directory()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        if self.multilingual:
            # å¤šè¯­è¨€ç›®å½•ç»“æ„
            (self.output_dir / "zh-cn").mkdir(exist_ok=True)
            (self.output_dir / "zh-cn" / "pages").mkdir(exist_ok=True)
            (self.output_dir / "en").mkdir(exist_ok=True)
            (self.output_dir / "en" / "pages").mkdir(exist_ok=True)
            (self.output_dir / "assets").mkdir(exist_ok=True)
        else:
            # å•è¯­è¨€ç›®å½•ç»“æ„
            (self.output_dir / "pages").mkdir(exist_ok=True)
            (self.output_dir / "assets").mkdir(exist_ok=True)
        
        # å­˜å‚¨å¤„ç†çš„é¡µé¢å’Œèµ„æº
        self.processed_pages = []
        self.downloaded_assets = []
        
        if self.use_selenium:
            self._setup_selenium()
    
    def _check_output_directory(self):
        """æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦ä¸ºç©ºï¼Œå¦‚æœä¸ä¸ºç©ºåˆ™æç¤ºç”¨æˆ·"""
        if not self.output_dir.exists():
            # ç›®å½•ä¸å­˜åœ¨ï¼Œå¯ä»¥å®‰å…¨åˆ›å»º
            return
        
        # æ£€æŸ¥ç›®å½•æ˜¯å¦ä¸ºç©ºï¼ˆå¿½ç•¥éšè—æ–‡ä»¶å’Œå¸¸è§çš„ç³»ç»Ÿæ–‡ä»¶ï¼‰
        existing_files = []
        ignore_patterns = {'.DS_Store', 'Thumbs.db', '.gitkeep', '.gitignore'}
        
        try:
            for item in self.output_dir.iterdir():
                if item.name not in ignore_patterns and not item.name.startswith('.'):
                    existing_files.append(item.name)
        except PermissionError:
            logger.warning(f"âš ï¸ æ— æ³•è®¿é—®ç›®å½• {self.output_dir}ï¼Œæƒé™ä¸è¶³")
            return
        
        if existing_files:
            if not self.force_overwrite:
                logger.error(f"âŒ è¾“å‡ºç›®å½• {self.output_dir} ä¸ä¸ºç©º")
                logger.error(f"ğŸ“ å‘ç°ä»¥ä¸‹æ–‡ä»¶/ç›®å½•: {', '.join(existing_files[:5])}")
                if len(existing_files) > 5:
                    logger.error(f"   ... è¿˜æœ‰ {len(existing_files) - 5} ä¸ªå…¶ä»–é¡¹ç›®")
                logger.error("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
                logger.error("   1. é€‰æ‹©ä¸€ä¸ªç©ºç›®å½•ä½œä¸ºè¾“å‡ºç›®å½•")
                logger.error("   2. æ‰‹åŠ¨æ¸…ç©ºå½“å‰ç›®å½•")
                logger.error("   3. ä½¿ç”¨ --force å‚æ•°å¼ºåˆ¶è¦†ç›–ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰")
                raise Exception(f"è¾“å‡ºç›®å½•ä¸ä¸ºç©º: {self.output_dir}")
            else:
                logger.warning(f"âš ï¸ è¾“å‡ºç›®å½• {self.output_dir} ä¸ä¸ºç©ºï¼Œå¯ç”¨å¼ºåˆ¶è¦†ç›–æ¨¡å¼")
                logger.warning(f"ğŸ“ å‘ç°ç°æœ‰æ–‡ä»¶: {', '.join(existing_files[:3])}")
                if len(existing_files) > 3:
                    logger.warning(f"   ... ç­‰ {len(existing_files)} ä¸ªé¡¹ç›®")
                
                # å¼ºåˆ¶æ¨¡å¼ï¼šæ¸…ç©ºç›®å½•
                logger.info("ğŸ—‘ï¸ æ­£åœ¨æ¸…ç©ºè¾“å‡ºç›®å½•...")
                self._clear_directory()
                logger.info("âœ… ç›®å½•å·²æ¸…ç©º")
        else:
            logger.info(f"âœ… è¾“å‡ºç›®å½• {self.output_dir} ä¸ºç©ºï¼Œå¯ä»¥å®‰å…¨ä½¿ç”¨")
    
    def _clear_directory(self):
        """æ¸…ç©ºè¾“å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶å’Œå­ç›®å½•"""
        import shutil
        
        ignore_patterns = {'.DS_Store', 'Thumbs.db', '.gitkeep', '.gitignore'}
        
        try:
            for item in self.output_dir.iterdir():
                # è·³è¿‡éšè—æ–‡ä»¶å’Œç³»ç»Ÿæ–‡ä»¶
                if item.name in ignore_patterns or item.name.startswith('.'):
                    continue
                
                try:
                    if item.is_dir():
                        shutil.rmtree(item)
                        logger.debug(f"ğŸ—‘ï¸ åˆ é™¤ç›®å½•: {item.name}")
                    else:
                        item.unlink()
                        logger.debug(f"ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶: {item.name}")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ— æ³•åˆ é™¤ {item.name}: {e}")
                    
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºç›®å½•å¤±è´¥: {e}")
            raise Exception(f"æ— æ³•æ¸…ç©ºè¾“å‡ºç›®å½•: {e}")
    
    def _setup_selenium(self):
        """è®¾ç½® Selenium WebDriverï¼ˆä¼˜å…ˆä½¿ç”¨Edgeï¼Œå¤‡é€‰Chromeï¼‰ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶"""
        max_retries = 3
        retry_delay = 2  # é‡è¯•é—´éš”ç§’æ•°
        
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ”„ ç¬¬ {attempt + 1}/{max_retries} æ¬¡å°è¯•å¯åŠ¨ Selenium WebDriver...")
                
                # é¦–å…ˆå°è¯•ä½¿ç”¨ Microsoft Edge
                edge_success = self._try_setup_edge(attempt + 1)
                if edge_success:
                    return
                
                # å¦‚æœEdgeå¤±è´¥ï¼Œå°è¯•Chrome
                chrome_success = self._try_setup_chrome(attempt + 1)
                if chrome_success:
                    return
                
                # å¦‚æœè¿™æ¬¡å°è¯•å¤±è´¥ï¼Œä½†è¿˜æœ‰é‡è¯•æœºä¼š
                if attempt < max_retries - 1:
                    logger.warning(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼Œ{retry_delay} ç§’åé‡è¯•...")
                    time.sleep(retry_delay)
                    retry_delay *= 1.5  # é€æ¸å¢åŠ é‡è¯•é—´éš”
                else:
                    # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥äº†
                    logger.error("âŒ æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼ŒSelenium å¯åŠ¨å¤±è´¥")
                    raise Exception(f"ç»è¿‡ {max_retries} æ¬¡é‡è¯•ï¼Œä»æ— æ³•å¯åŠ¨ Selenium WebDriver")
                    
            except Exception as e:
                if attempt < max_retries - 1:
                    logger.warning(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡å°è¯•å¼‚å¸¸: {e}")
                    logger.info(f"ğŸ”„ {retry_delay} ç§’åè¿›è¡Œç¬¬ {attempt + 2} æ¬¡é‡è¯•...")
                    time.sleep(retry_delay)
                    retry_delay *= 1.5
                else:
                    logger.error(f"âŒ æœ€ç»ˆå°è¯•å¤±è´¥: {e}")
                    raise Exception(f"ç»è¿‡ {max_retries} æ¬¡é‡è¯•ï¼ŒSelenium WebDriver å¯åŠ¨å¤±è´¥: {e}")

    def _try_setup_edge(self, attempt_num: int) -> bool:
        """å°è¯•è®¾ç½® Edge WebDriver"""
        try:
            logger.info(f"ğŸ”„ ç¬¬ {attempt_num} æ¬¡å°è¯•å¯åŠ¨ Microsoft Edge WebDriver...")
            edge_options = EdgeOptions()
            edge_options.add_argument("--headless")
            edge_options.add_argument("--no-sandbox")
            edge_options.add_argument("--disable-dev-shm-usage")
            edge_options.add_argument("--disable-gpu")
            edge_options.add_argument("--window-size=1920,1080")
            edge_options.add_argument("--disable-blink-features=AutomationControlled")
            edge_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            edge_options.add_experimental_option('useAutomationExtension', False)
            # æ·»åŠ æ›´å¤šé€‰é¡¹æ¥å¤„ç†ç½‘ç»œè¿æ¥
            edge_options.add_argument("--disable-web-security")
            edge_options.add_argument("--allow-running-insecure-content")
            edge_options.add_argument("--disable-extensions")
            edge_options.add_argument("--disable-plugins")
            edge_options.add_argument("--disable-images")  # åŠ å¿«åŠ è½½é€Ÿåº¦
            edge_options.add_argument("--disable-javascript-harmony-shipping")
            edge_options.add_argument("--disable-background-timer-throttling")
            edge_options.add_argument("--disable-renderer-backgrounding")
            edge_options.add_argument("--disable-backgrounding-occluded-windows")
            edge_options.add_argument("--disable-client-side-phishing-detection")
            edge_options.add_argument("--disable-sync")
            edge_options.add_argument("--disable-translate")
            edge_options.add_argument("--hide-scrollbars")
            edge_options.add_argument("--mute-audio")
            
            try:
                # å°è¯•è‡ªåŠ¨ä¸‹è½½å¹¶è®¾ç½® EdgeDriver
                edge_service = EdgeService(EdgeChromiumDriverManager().install())
            except Exception as download_error:
                logger.warning(f"æ— æ³•ä¸‹è½½EdgeDriver: {download_error}")
                # å°è¯•ä½¿ç”¨ç³»ç»Ÿå·²å®‰è£…çš„EdgeDriver
                try:
                    edge_service = EdgeService()  # ä½¿ç”¨é»˜è®¤è·¯å¾„
                except:
                    raise Exception("EdgeDriverä¸å¯ç”¨ï¼Œæ— æ³•ä¸‹è½½ä¹Ÿæ— æ³•åœ¨ç³»ç»Ÿä¸­æ‰¾åˆ°")
            
            self.driver = webdriver.Edge(service=edge_service, options=edge_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            # è®¾ç½®è¶…æ—¶
            self.driver.set_page_load_timeout(60)  # å¢åŠ é¡µé¢åŠ è½½è¶…æ—¶
            self.driver.implicitly_wait(15)        # å¢åŠ éšå¼ç­‰å¾…æ—¶é—´
            
            logger.info("ğŸš€ Microsoft Edge WebDriver å·²å¯åŠ¨")
            return True
            
        except Exception as edge_error:
            logger.warning(f"Edge ç¬¬ {attempt_num} æ¬¡å¯åŠ¨å¤±è´¥: {edge_error}")
            return False

    def _try_setup_chrome(self, attempt_num: int) -> bool:
        """å°è¯•è®¾ç½® Chrome WebDriver"""
        try:
            logger.info(f"ğŸ”„ ç¬¬ {attempt_num} æ¬¡å°è¯•å¯åŠ¨ Chrome WebDriver...")
            chrome_options = ChromeOptions()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_argument("--disable-web-security")
            chrome_options.add_argument("--allow-running-insecure-content")
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--disable-plugins")
            chrome_options.add_argument("--disable-images")
            chrome_options.add_argument("--disable-javascript-harmony-shipping")
            chrome_options.add_argument("--disable-background-timer-throttling")
            chrome_options.add_argument("--disable-renderer-backgrounding")
            chrome_options.add_argument("--disable-backgrounding-occluded-windows")
            chrome_options.add_argument("--disable-client-side-phishing-detection")
            chrome_options.add_argument("--disable-sync")
            chrome_options.add_argument("--disable-translate")
            chrome_options.add_argument("--hide-scrollbars")
            chrome_options.add_argument("--mute-audio")
            
            try:
                # å°è¯•è‡ªåŠ¨ä¸‹è½½å¹¶è®¾ç½® ChromeDriver
                chrome_service = ChromeService(ChromeDriverManager().install())
            except Exception as download_error:
                logger.warning(f"æ— æ³•ä¸‹è½½ChromeDriver: {download_error}")
                # å°è¯•ä½¿ç”¨ç³»ç»Ÿå·²å®‰è£…çš„ChromeDriver
                try:
                    chrome_service = ChromeService()  # ä½¿ç”¨é»˜è®¤è·¯å¾„
                except:
                    raise Exception("ChromeDriverä¸å¯ç”¨ï¼Œæ— æ³•ä¸‹è½½ä¹Ÿæ— æ³•åœ¨ç³»ç»Ÿä¸­æ‰¾åˆ°")
            
            self.driver = webdriver.Chrome(service=chrome_service, options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            # è®¾ç½®è¶…æ—¶
            self.driver.set_page_load_timeout(60)
            self.driver.implicitly_wait(15)
            
            logger.info("ğŸš€ Chrome WebDriver å·²å¯åŠ¨")
            return True
            
        except Exception as chrome_error:
            logger.warning(f"Chrome ç¬¬ {attempt_num} æ¬¡å¯åŠ¨å¤±è´¥: {chrome_error}")
            return False
    
    def _get_page_content(self, url: str) -> str:
        """è·å–é¡µé¢å†…å®¹"""
        if self.use_selenium and self.driver:
            return self._get_page_with_selenium(url)
        else:
            return self._get_page_with_requests(url)
    
    def _get_page_with_selenium(self, url: str) -> str:
        """ä½¿ç”¨ Selenium è·å–é¡µé¢å†…å®¹ï¼ˆç­‰å¾…åŠ¨æ€åŠ è½½ï¼‰"""
        try:
            logger.info(f"ğŸ”„ æ­£åœ¨åŠ è½½é¡µé¢ï¼ˆSeleniumï¼‰: {url}")
            self.driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ç­‰å¾…æ›´é•¿æ—¶é—´è®©å†…å®¹åŠ è½½
            logger.info("â³ ç­‰å¾…JavaScriptæ‰§è¡Œå®Œæˆ...")
            time.sleep(8)  # å¢åŠ ç­‰å¾…æ—¶é—´
            
            # å°è¯•ç­‰å¾…å†…å®¹åŠ è½½å®Œæˆï¼ˆæ£€æŸ¥æ˜¯å¦è¿˜æœ‰ Loading... æ–‡æœ¬ï¼‰
            try:
                WebDriverWait(self.driver, 15).until_not(
                    EC.text_to_be_present_in_element((By.TAG_NAME, "body"), "Loading...")
                )
                logger.info("âœ… åŠ¨æ€å†…å®¹å·²åŠ è½½")
            except:
                logger.warning("âš ï¸ é¡µé¢å¯èƒ½ä»åœ¨åŠ è½½ä¸­ï¼Œç»§ç»­å¤„ç†")
            
            # é¢å¤–ç­‰å¾…ç¡®ä¿æ‰€æœ‰å¼‚æ­¥å†…å®¹éƒ½åŠ è½½å®Œæˆ
            logger.info("ğŸ” ç­‰å¾…å¼‚æ­¥å†…å®¹åŠ è½½...")
            time.sleep(5)
            
            # å°è¯•è§¦å‘ä¸€äº›ç”¨æˆ·äº¤äº’æ¥ç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½è¢«æ¸²æŸ“
            try:
                # æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ‡’åŠ è½½å†…å®¹
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                self.driver.execute_script("window.scrollTo(0, 0);")
                time.sleep(2)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯¼èˆªèœå•å¹¶å°è¯•å±•å¼€
                nav_elements = self.driver.find_elements(By.CSS_SELECTOR, 
                    "[class*='nav'], [class*='menu'], [class*='sidebar'], [role='navigation']")
                if nav_elements:
                    logger.info(f"ğŸ” å‘ç° {len(nav_elements)} ä¸ªå¯¼èˆªå…ƒç´ ")
                    for nav in nav_elements[:3]:  # åªå¤„ç†å‰3ä¸ª
                        try:
                            # å°è¯•ç‚¹å‡»å¯èƒ½çš„å±•å¼€æŒ‰é’®
                            expandable = nav.find_elements(By.CSS_SELECTOR, 
                                "[aria-expanded='false'], .collapsed, .expand-btn")
                            for btn in expandable:
                                if btn.is_displayed():
                                    btn.click()
                                    time.sleep(1)
                        except:
                            pass
                            
            except Exception as e:
                logger.debug(f"äº¤äº’æ“ä½œå¤±è´¥: {e}")
            
            # è·å–æœ€ç»ˆçš„é¡µé¢æºç 
            page_source = self.driver.page_source
            logger.info(f"ğŸ“„ è·å–é¡µé¢æºç : {len(page_source)} å­—ç¬¦")
            
            return page_source
            
        except Exception as e:
            logger.error(f"Selenium è·å–é¡µé¢å¤±è´¥: {e}")
            return self._get_page_with_requests(url)
    
    def _get_page_with_requests(self, url: str) -> str:
        """ä½¿ç”¨ requests è·å–é¡µé¢å†…å®¹"""
        try:
            logger.info(f"ğŸ“¡ æ­£åœ¨è·å–é¡µé¢: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logger.error(f"è·å–é¡µé¢å¤±è´¥: {e}")
            return ""
    
    def _safe_decode_unicode(self, text):
        """æ”¹è¿›çš„ Unicode è§£ç ï¼Œæ›´å¥½åœ°å¤„ç†è½¬ä¹‰å­—ç¬¦"""
        try:
            # é¦–å…ˆå¤„ç†å¸¸è§çš„è½¬ä¹‰åºåˆ—
            text = text.replace('\\n', '\n')
            text = text.replace('\\t', '\t')
            text = text.replace('\\r', '\r')
            text = text.replace('\\"', '"')
            text = text.replace('\\/', '/')
            text = text.replace('\\\\', '\\')  # å¤„ç†åŒåæ–œæ 
            
            # å¤„ç† Unicode è½¬ä¹‰
            text = text.replace('\\u003c', '<')
            text = text.replace('\\u003e', '>')
            text = text.replace('\\u0026', '&')
            text = text.replace('\\u0027', "'")
            
            # å¤„ç†å…¶ä»– Unicode è½¬ä¹‰åºåˆ—
            def replace_unicode(match):
                try:
                    return chr(int(match.group(1), 16))
                except:
                    return match.group(0)
            
            text = re.sub(r'\\u([0-9a-fA-F]{4})', replace_unicode, text)
            
            return text
        except Exception as e:
            logger.debug(f"Unicode è§£ç å¤±è´¥: {e}")
            return text
    
    def _extract_nextjs_content(self, html_content: str, navigation_links: dict = None) -> tuple:
        """ä» Next.js çš„å¼‚æ­¥å†…å®¹ä¸­æå–é¡µé¢å’Œå¯¼èˆªç»“æ„"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # å­˜å‚¨é¡µé¢ç‰‡æ®µ
        page_fragments = {}
        navigation_structure = []
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«å†…å®¹çš„è„šæœ¬
        scripts = soup.find_all('script')
        
        logger.info(f"ğŸ” åˆ†æ {len(scripts)} ä¸ªè„šæœ¬æ ‡ç­¾...")
        
        for i, script in enumerate(scripts):
            if script.string and 'self.__next_f.push' in script.string:
                script_content = script.string
                
                # æå–å¯¼èˆªç»“æ„
                nav_structure = self._extract_navigation_structure(script_content)
                if nav_structure:
                    navigation_structure.extend(nav_structure)
                
                # æå–è¿™ä¸ªè„šæœ¬ä¸­çš„æ‰€æœ‰å†…å®¹ç‰‡æ®µ
                fragments = self._extract_all_content_fragments(script_content, navigation_links)
                
                for fragment in fragments:
                    title = fragment['title']
                    content = fragment['content']
                    original_filename = fragment.get('original_filename')
                    
                    if title not in page_fragments:
                        page_fragments[title] = {
                            'contents': [],
                            'original_filename': original_filename
                        }
                    
                    page_fragments[title]['contents'].append(content)
                    # å¦‚æœæœ‰åŸå§‹æ–‡ä»¶åï¼Œä¿ç•™ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„
                    if original_filename and not page_fragments[title]['original_filename']:
                        page_fragments[title]['original_filename'] = original_filename
        
        # åˆå¹¶æ¯ä¸ªé¡µé¢çš„æ‰€æœ‰ç‰‡æ®µ
        final_pages = []
        for title, page_data in page_fragments.items():
            # åˆå¹¶å†…å®¹
            merged_content = '\n'.join(page_data['contents'])
            
            # æ¸…ç†åˆå¹¶åçš„å†…å®¹
            cleaned_content = self._clean_merged_content(merged_content)
            
            if cleaned_content and len(cleaned_content) > 100:
                original_filename = page_data.get('original_filename')
                slug = self._generate_slug(title, original_filename)
                
                final_pages.append({
                    'title': title,
                    'content': cleaned_content,
                    'slug': slug,
                    'original_filename': original_filename,
                    'order': self._get_page_order_from_nav(title, navigation_structure)
                })
        
        logger.info(f"ğŸ“„ æå–åˆ° {len(final_pages)} ä¸ªå®Œæ•´é¡µé¢")
        logger.info(f"ğŸ“‹ æå–åˆ°å¯¼èˆªç»“æ„: {len(navigation_structure)} ä¸ªæ¡ç›®")
        
        return final_pages, navigation_structure
    
    def _extract_all_content_fragments(self, script_content: str, navigation_links: dict = None) -> list:
        """æå–è„šæœ¬ä¸­çš„æ‰€æœ‰å†…å®¹ç‰‡æ®µï¼Œæ­£ç¡®å¤„ç†åµŒå¥—å¼•å·"""
        fragments = []
        
        # ä½¿ç”¨æ›´æ™ºèƒ½çš„æ–¹æ³•æ¥æå–å†…å®¹ï¼Œæ­£ç¡®å¤„ç†åµŒå¥—å¼•å·
        pattern = r'self\.__next_f\.push\(\[1,"'
        matches = list(re.finditer(pattern, script_content))
        
        for match in matches:
            start_pos = match.end()
            
            # ä»è¿™ä¸ªä½ç½®å¼€å§‹ï¼Œæ‰¾åˆ°åŒ¹é…çš„ç»“æŸå¼•å·
            content = ""
            pos = start_pos
            escape_next = False
            
            while pos < len(script_content):
                char = script_content[pos]
                
                if escape_next:
                    content += char
                    escape_next = False
                elif char == '\\':
                    content += char
                    escape_next = True
                elif char == '"':
                    # æ‰¾åˆ°ç»“æŸå¼•å·
                    break
                else:
                    content += char
                
                pos += 1
            
            # å®‰å…¨è§£ç 
            decoded_content = self._safe_decode_unicode(content)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆå†…å®¹
            if len(decoded_content.strip()) > 20:
                # å°è¯•è¯†åˆ«æ ‡é¢˜å’ŒåŸå§‹æ–‡ä»¶å
                title = self._extract_title_from_content(decoded_content)
                original_filename = self._extract_original_filename(decoded_content)
                
                if title:
                    # ä¼˜å…ˆä»å¯¼èˆªé“¾æ¥ä¸­è·å–çœŸå®æ–‡ä»¶å
                    real_filename = None
                    if navigation_links and title in navigation_links:
                        real_filename = navigation_links[title]
                        logger.info(f"ğŸ¯ ä½¿ç”¨å¯¼èˆªé“¾æ¥æ–‡ä»¶å: {title} -> {real_filename}")
                    else:
                        # å¦‚æœå¯¼èˆªé“¾æ¥ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»å†…å®¹ä¸­æå–
                        real_filename = self._extract_original_filename(decoded_content)
                        if real_filename:
                            logger.info(f"ğŸ” ä»å†…å®¹æå–æ–‡ä»¶å: {title} -> {real_filename}")
                        else:
                            logger.debug(f"ğŸ” æœªæ‰¾åˆ°åŸå§‹æ–‡ä»¶å: {title}")
                    
                    fragments.append({
                        'title': title,
                        'content': decoded_content,
                        'original_filename': real_filename
                    })
        
        return fragments
    
    def _extract_navigation_structure(self, script_content: str) -> list:
        """æå– DeepWiki çš„å¯¼èˆªç»“æ„"""
        navigation = []
        
        try:
            # ä¸“é—¨æœç´¢åŒ…å«è·¯ç”±ä¿¡æ¯çš„æ•°æ®ç»“æ„
            # è¿™äº›å¯èƒ½åŒ…å«çœŸå®çš„æ–‡ä»¶å
            route_patterns = [
                # æœç´¢è·¯ç”±é…ç½®
                r'"routes?":\s*\[([^\]]+)\]',
                r'"paths?":\s*\[([^\]]+)\]',
                r'"links?":\s*\[([^\]]+)\]',
                # æœç´¢é¡µé¢é…ç½®
                r'"pages":\s*\[([^\]]+)\]',
                r'"navigation":\s*\[([^\]]+)\]',
                r'"sidebar":\s*\[([^\]]+)\]',
                r'"menuItems":\s*\[([^\]]+)\]',
                # æœç´¢Next.jsè·¯ç”±æ•°æ®
                r'"__NEXT_DATA__"[^{]*{[^}]*"page"[^}]*}',
                # æœç´¢åŒ…å«hrefæˆ–pathçš„å¯¹è±¡
                r'\{[^}]*"(?:href|path|route)":\s*"[^"]*\/([0-9]+(?:\.[0-9]+)?-[a-zA-Z][a-zA-Z0-9-]*)"[^}]*\}',
            ]
            
            logger.debug(f"ğŸ” åœ¨ {len(script_content)} å­—ç¬¦çš„è„šæœ¬ä¸­æœç´¢å¯¼èˆªç»“æ„...")
            
            for pattern in route_patterns:
                matches = re.finditer(pattern, script_content, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    logger.debug(f"ğŸ“ æ‰¾åˆ°æ¨¡å¼åŒ¹é…: {pattern[:50]}...")
                    if match.groups():
                        nav_content = match.group(1) if len(match.groups()) >= 1 else match.group(0)
                        nav_items = self._parse_navigation_items(nav_content)
                        if nav_items:
                            navigation.extend(nav_items)
                            logger.info(f"ğŸ“‹ ä»æ¨¡å¼ä¸­æå–åˆ° {len(nav_items)} ä¸ªå¯¼èˆªé¡¹")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„å¯¼èˆªç»“æ„ï¼Œå°è¯•ä»é¡µé¢é¡ºåºæ¨æ–­
            if not navigation:
                logger.debug("ğŸ” å°è¯•ä»å†…å®¹æ¨æ–­å¯¼èˆªç»“æ„...")
                navigation = self._infer_navigation_from_content(script_content)
                
        except Exception as e:
            logger.debug(f"æå–å¯¼èˆªç»“æ„å¤±è´¥: {e}")
        
        return navigation
    
    def _parse_navigation_items(self, nav_content: str) -> list:
        """è§£æå¯¼èˆªé¡¹ç›®ï¼Œç‰¹åˆ«å…³æ³¨æ–‡ä»¶åä¿¡æ¯"""
        items = []
        try:
            # é¦–å…ˆæœç´¢åŒ…å«è·¯å¾„ä¿¡æ¯çš„å®Œæ•´å¯¹è±¡
            filename_objects = re.findall(
                r'\{[^}]*(?:"title":\s*"([^"]+)")[^}]*(?:"(?:href|path|route)":\s*"[^"]*\/([0-9]+(?:\.[0-9]+)?-[a-zA-Z][a-zA-Z0-9-]*)"[^}]*|[^}]*"(?:href|path|route)":\s*"[^"]*\/([0-9]+(?:\.[0-9]+)?-[a-zA-Z][a-zA-Z0-9-]*)"[^}]*"title":\s*"([^"]+)")[^}]*\}',
                nav_content,
                re.IGNORECASE
            )
            
            if filename_objects:
                logger.info(f"ğŸ¯ æ‰¾åˆ°åŒ…å«æ–‡ä»¶åçš„å¯¼èˆªå¯¹è±¡: {len(filename_objects)} ä¸ª")
                for match in filename_objects:
                    title = match[0] or match[3]  # titleå¯èƒ½åœ¨ä¸åŒä½ç½®
                    filename = match[1] or match[2]  # filenameä¹Ÿå¯èƒ½åœ¨ä¸åŒä½ç½®
                    if title and filename:
                        items.append({
                            'title': title,
                            'filename': filename,
                            'order': len(items),
                            'level': 0
                        })
                        logger.info(f"ğŸ“ å¯¼èˆªé¡¹: {title} -> {filename}")
            
            # å¦‚æœæ²¡æ‰¾åˆ°å¸¦æ–‡ä»¶åçš„ï¼Œå°è¯•ä¼ ç»Ÿçš„è§£æ
            if not items:
                # å°è¯•è§£æ JSON æ ¼å¼çš„å¯¼èˆªé¡¹
                # ç®€åŒ–å¤„ç†ï¼ŒæŸ¥æ‰¾æ ‡é¢˜å’Œé¡ºåºä¿¡æ¯
                title_pattern = r'"title":\s*"([^"]+)"'
                order_pattern = r'"order":\s*(\d+)'
                level_pattern = r'"level":\s*(\d+)'
                
                titles = re.findall(title_pattern, nav_content)
                orders = re.findall(order_pattern, nav_content)
                levels = re.findall(level_pattern, nav_content)
                
                for i, title in enumerate(titles):
                    item = {
                        'title': title,
                        'order': int(orders[i]) if i < len(orders) else i,
                        'level': int(levels[i]) if i < len(levels) else 0
                    }
                    items.append(item)
                    
        except Exception as e:
            logger.debug(f"è§£æå¯¼èˆªé¡¹å¤±è´¥: {e}")
        
        return items
    
    def _infer_navigation_from_content(self, script_content: str) -> list:
        """ä»å†…å®¹ä¸­æ¨æ–­å¯¼èˆªç»“æ„"""
        items = []
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰é¡µé¢æ ‡é¢˜çš„å‡ºç°é¡ºåº
            title_pattern = r'# ([^#\n]+)'
            matches = re.finditer(title_pattern, script_content)
            
            order = 0
            for match in matches:
                title = match.group(1).strip()
                if len(title) > 2 and not title.startswith('#'):
                    items.append({
                        'title': title,
                        'order': order,
                        'level': 0
                    })
                    order += 1
                    
        except Exception as e:
            logger.debug(f"æ¨æ–­å¯¼èˆªç»“æ„å¤±è´¥: {e}")
        
        return items
    
    def _get_page_order_from_nav(self, page_title: str, navigation: list) -> int:
        """ä»å¯¼èˆªç»“æ„ä¸­è·å–é¡µé¢é¡ºåº"""
        for nav_item in navigation:
            if nav_item['title'] == page_title:
                return nav_item['order']
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›è¾ƒå¤§çš„æ•°å­—ï¼Œæ’åœ¨æœ€å
        return 9999
    
    def _extract_title_from_content(self, content: str) -> str:
        """ä»å†…å®¹ä¸­æå–æ ‡é¢˜"""
        lines = content.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if line.startswith('# ') and not line.startswith('# #'):
                return line.lstrip('#').strip()
        
        return None
    
    def _process_sources_links(self, content: str, github_info: dict) -> str:
        """å¤„ç† Sources é“¾æ¥ï¼Œè¡¥å…¨ GitHub åœ°å€"""
        if not github_info or not github_info.get('repo_url'):
            return content
        
        repo_url = github_info['repo_url']
        commit_sha = github_info.get('commit_sha', 'main')
        
        # åŒ¹é… Sources è¡Œä¸­çš„é“¾æ¥æ ¼å¼ï¼š[filename:line1-line2]() æˆ– [filename:line1]()
        sources_pattern = r'Sources:\s*(.+?)(?=\n|$)'
        link_pattern = r'\[([^:]+):(\d+)(?:-(\d+))?\]\(\)'
        
        def replace_sources_line(match):
            sources_line = match.group(1)
            
            def replace_single_link(link_match):
                filename = link_match.group(1)
                start_line = link_match.group(2)
                end_line = link_match.group(3)  # å¯èƒ½ä¸º None
                
                # æ„é€  GitHub é“¾æ¥
                if end_line:
                    # æœ‰ç»“æŸè¡Œå·ï¼š[filename:start-end]
                    github_link = f"{repo_url}/blob/{commit_sha}/{filename}#L{start_line}-L{end_line}"
                    return f"[{filename}:{start_line}-{end_line}]({github_link})"
                else:
                    # åªæœ‰å•è¡Œï¼š[filename:line]
                    github_link = f"{repo_url}/blob/{commit_sha}/{filename}#L{start_line}"
                    return f"[{filename}:{start_line}]({github_link})"
            
            updated_line = re.sub(link_pattern, replace_single_link, sources_line)
            return f"Sources: {updated_line}"
        
        # æ›¿æ¢æ‰€æœ‰ Sources è¡Œ
        processed_content = re.sub(sources_pattern, replace_sources_line, content, flags=re.MULTILINE)
        
        return processed_content

    def _extract_dynamic_navigation_data(self, html_content: str) -> dict:
        """ä»åŠ¨æ€åŠ è½½çš„é¡µé¢ä¸­æå–çœŸå®çš„å¯¼èˆªæ•°æ®"""
        navigation_data = {}
        
        if not self.use_selenium or not self.driver:
            logger.debug("Seleniumæœªå¯ç”¨ï¼Œè·³è¿‡åŠ¨æ€å¯¼èˆªæ•°æ®æå–")
            return navigation_data
        
        try:
            logger.info("ğŸ” åˆ†æåŠ¨æ€å¯¼èˆªæ•°æ®...")
            
            # 1. å°è¯•ä»JavaScriptæ‰§è¡Œä¸Šä¸‹æ–‡ä¸­è·å–è·¯ç”±ä¿¡æ¯
            try:
                # æ£€æŸ¥React Routeræ•°æ®
                router_data = self.driver.execute_script("""
                    // å°è¯•è·å–React Routeræˆ–Next.jsè·¯ç”±æ•°æ®
                    if (window.__NEXT_DATA__) {
                        return window.__NEXT_DATA__;
                    }
                    
                    // å°è¯•è·å–ReactçŠ¶æ€
                    if (window.React && window.React.Component) {
                        const components = document.querySelectorAll('[data-reactroot]');
                        for (let comp of components) {
                            if (comp._reactInternalFiber || comp._reactInternalInstance) {
                                // æœ‰Reactç»„ä»¶
                                return 'REACT_FOUND';
                            }
                        }
                    }
                    
                    // æŸ¥æ‰¾å¯èƒ½çš„å¯¼èˆªæ•°æ®
                    const navElements = document.querySelectorAll('nav, [role="navigation"], .sidebar, .nav-menu');
                    const navData = [];
                    for (let nav of navElements) {
                        const links = nav.querySelectorAll('a[href]');
                        for (let link of links) {
                            const href = link.getAttribute('href');
                            const text = link.textContent.trim();
                            if (href && text && href.includes('-')) {
                                navData.push({href: href, text: text});
                            }
                        }
                    }
                    return navData.length > 0 ? navData : null;
                """)
                
                if router_data:
                    logger.info(f"ğŸ¯ è·å–åˆ°JavaScriptè·¯ç”±æ•°æ®: {type(router_data)}")
                    if isinstance(router_data, list):
                        # å¤„ç†å¯¼èˆªé“¾æ¥æ•°æ®
                        for item in router_data:
                            if isinstance(item, dict) and 'href' in item and 'text' in item:
                                href = item['href']
                                text = item['text']
                                # æå–æ–‡ä»¶å
                                filename = self._extract_filename_from_href(href)
                                if filename:
                                    navigation_data[text] = filename
                                    logger.info(f"ğŸ”— åŠ¨æ€å¯¼èˆª: {text} -> {filename}")
                    elif isinstance(router_data, dict):
                        # å¤„ç†Next.jsæ•°æ®
                        logger.debug(f"Next.jsæ•°æ®ç»“æ„: {list(router_data.keys())}")
                        # å¯ä»¥è¿›ä¸€æ­¥è§£æNext.jsçš„è·¯ç”±æ•°æ®
                        
            except Exception as e:
                logger.debug(f"JavaScriptæ‰§è¡Œå¤±è´¥: {e}")
            
            # 2. ç›´æ¥æ£€æŸ¥å½“å‰é¡µé¢çš„æ‰€æœ‰é“¾æ¥
            try:
                links = self.driver.find_elements(By.CSS_SELECTOR, "a[href]")
                logger.info(f"ğŸ” åˆ†æé¡µé¢ä¸­çš„ {len(links)} ä¸ªé“¾æ¥...")
                
                for link in links:
                    try:
                        href = link.get_attribute('href')
                        text = link.text.strip()
                        
                        if href and text and len(text) > 2:
                            # æ£€æŸ¥é“¾æ¥æ˜¯å¦åŒ…å«åºå·æ–‡ä»¶åæ¨¡å¼
                            filename = self._extract_filename_from_href(href)
                            if filename and re.match(r'^[0-9]+(\.[0-9]+)?-[a-zA-Z]', filename):
                                navigation_data[text] = filename
                                logger.info(f"ğŸ¯ æ‰¾åˆ°åºå·é“¾æ¥: {text} -> {filename}")
                                
                    except Exception as e:
                        logger.debug(f"å¤„ç†é“¾æ¥å¤±è´¥: {e}")
                        
            except Exception as e:
                logger.debug(f"é“¾æ¥åˆ†æå¤±è´¥: {e}")
            
            # 3. å°è¯•æ¨¡æ‹Ÿç”¨æˆ·æ“ä½œæ¥å±•å¼€æ›´å¤šå¯¼èˆª
            try:
                # æŸ¥æ‰¾å¯èƒ½çš„å±•å¼€æŒ‰é’®
                expand_buttons = self.driver.find_elements(By.CSS_SELECTOR, 
                    "[aria-expanded='false'], .expand, .collapse, .toggle-btn, .menu-toggle")
                
                for btn in expand_buttons[:5]:  # æœ€å¤šå°è¯•5ä¸ªæŒ‰é’®
                    try:
                        if btn.is_displayed() and btn.is_enabled():
                            logger.debug("ğŸ”„ å°è¯•å±•å¼€å¯¼èˆªèœå•...")
                            btn.click()
                            time.sleep(2)
                            
                            # é‡æ–°æ£€æŸ¥é“¾æ¥
                            new_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href]")
                            for link in new_links:
                                href = link.get_attribute('href')
                                text = link.text.strip()
                                
                                if href and text and text not in navigation_data:
                                    filename = self._extract_filename_from_href(href)
                                    if filename and re.match(r'^[0-9]+(\.[0-9]+)?-[a-zA-Z]', filename):
                                        navigation_data[text] = filename
                                        logger.info(f"ğŸ†• å±•å¼€åå‘ç°: {text} -> {filename}")
                                        
                    except Exception as e:
                        logger.debug(f"å±•å¼€æ“ä½œå¤±è´¥: {e}")
                        
            except Exception as e:
                logger.debug(f"å±•å¼€å¯¼èˆªå¤±è´¥: {e}")
                
        except Exception as e:
            logger.debug(f"åŠ¨æ€å¯¼èˆªæ•°æ®æå–å¤±è´¥: {e}")
        
        logger.info(f"ğŸ¯ åŠ¨æ€æå–åˆ° {len(navigation_data)} ä¸ªçœŸå®æ–‡ä»¶åæ˜ å°„")
        return navigation_data
    
    def _extract_filename_from_href(self, href: str) -> str:
        """ä»hrefä¸­æå–æ–‡ä»¶å"""
        if not href:
            return None
            
        try:
            # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹
            href = href.split('?')[0].split('#')[0]
            
            # è·å–è·¯å¾„çš„æœ€åä¸€æ®µ
            path_segments = href.strip('/').split('/')
            if path_segments:
                filename = path_segments[-1]
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„åºå·æ–‡ä»¶å
                if re.match(r'^[0-9]+(\.[0-9]+)?-[a-zA-Z][a-zA-Z0-9-]*$', filename):
                    return filename
                    
                # ä¹Ÿæ£€æŸ¥å€’æ•°ç¬¬äºŒæ®µ
                if len(path_segments) >= 2:
                    filename = path_segments[-2]
                    if re.match(r'^[0-9]+(\.[0-9]+)?-[a-zA-Z][a-zA-Z0-9-]*$', filename):
                        return filename
                        
        except Exception as e:
            logger.debug(f"æå–æ–‡ä»¶åå¤±è´¥: {e}")
        
        return None
    
    def _extract_navigation_links(self, html_content: str) -> dict:
        """ä»é¡µé¢å¯¼èˆªæ æå–çœŸæ­£çš„æ–‡ä»¶åé“¾æ¥"""
        title_to_filename = {}
        
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯¼èˆªé“¾æ¥
            # DeepWiki çš„å¯¼èˆªé“¾æ¥é€šå¸¸åœ¨å·¦ä¾§ä¾§è¾¹æ 
            nav_links = soup.find_all('a', href=True)
            
            logger.debug(f"ğŸ” åˆ†æ {len(nav_links)} ä¸ªé“¾æ¥...")
            
            # è°ƒè¯•ï¼šè¾“å‡ºå‰å‡ ä¸ªé“¾æ¥çš„è¯¦ç»†ä¿¡æ¯
            for i, link in enumerate(nav_links[:10]):  # åªçœ‹å‰10ä¸ª
                href = link.get('href', '')
                title = link.get_text(strip=True)
                classes = link.get('class', [])
                logger.debug(f"ğŸ”— é“¾æ¥ {i+1}: href='{href}', title='{title}', classes={classes}")
            
            for link in nav_links:
                href = link.get('href', '')
                title = link.get_text(strip=True)
                
                # æ›´çµæ´»çš„é“¾æ¥æ£€æŸ¥
                # æ£€æŸ¥æ˜¯å¦åŒ…å«åºå·æ–‡ä»¶åæ¨¡å¼
                if href and title:
                    # æå– href ä¸­çš„æœ€åä¸€ä¸ªè·¯å¾„æ®µ
                    path_segments = href.strip('/').split('/')
                    if path_segments:
                        filename = path_segments[-1]
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„åºå·æ–‡ä»¶å
                        if re.match(r'^[0-9]+(\.[0-9]+)?-[a-zA-Z][a-zA-Z0-9-]*$', filename):
                            title_to_filename[title] = filename
                            logger.info(f"ğŸ”— æ‰¾åˆ°å¯¼èˆªé“¾æ¥: {title} -> {filename}")
                        else:
                            # è°ƒè¯•ï¼šè®°å½•æ‰€æœ‰æ‰¾åˆ°çš„é“¾æ¥
                            logger.debug(f"ğŸ“ æ£€æŸ¥é“¾æ¥: {title} -> {href} (æ–‡ä»¶å: {filename})")
            
            if not title_to_filename:
                # å¦‚æœæ²¡æ‰¾åˆ°ä»»ä½•åºå·é“¾æ¥ï¼Œå°è¯•æ›´å®½æ¾çš„æœç´¢
                logger.debug("ğŸ” å°è¯•å®½æ¾æœç´¢...")
                for link in nav_links:
                    href = link.get('href', '')
                    title = link.get_text(strip=True)
                    
                    # æ£€æŸ¥ä»»ä½•åŒ…å«è¿å­—ç¬¦çš„è·¯å¾„æ®µ
                    if href and title and '-' in href:
                        path_segments = href.strip('/').split('/')
                        for segment in path_segments:
                            if re.match(r'^[0-9]+.*-[a-zA-Z]', segment):
                                title_to_filename[title] = segment
                                logger.info(f"ğŸ”— å®½æ¾åŒ¹é…å¯¼èˆªé“¾æ¥: {title} -> {segment}")
                                break
            
            logger.info(f"ğŸ“‹ ä»å¯¼èˆªæ æå–åˆ° {len(title_to_filename)} ä¸ªæ–‡ä»¶åæ˜ å°„")
            return title_to_filename
            
        except Exception as e:
            logger.error(f"æå–å¯¼èˆªé“¾æ¥å¤±è´¥: {e}")
            return {}
    
    def _extract_original_filename(self, content: str) -> str:
        """ä»å†…å®¹ä¸­æå–åŸå§‹æ–‡ä»¶å"""
        try:
            # é¦–å…ˆè¿›è¡Œå¹¿æ³›æœç´¢ï¼Œçœ‹çœ‹å†…å®¹ä¸­æœ‰ä»€ä¹ˆæ•°å­—-è¿å­—ç¬¦æ¨¡å¼
            broad_search = re.findall(r'\b[0-9]+[-\.][a-zA-Z][a-zA-Z0-9-]*\b', content)
            if broad_search:
                logger.debug(f"ğŸ” å¹¿æ³›æœç´¢æ‰¾åˆ°çš„æ¨¡å¼: {broad_search[:10]}")  # åªæ˜¾ç¤ºå‰10ä¸ª
            
            # 1. é¦–å…ˆæœç´¢æ ‡å‡†çš„åºå·æ–‡ä»¶åæ¨¡å¼ï¼ˆå¦‚ "1-overview", "4.1-backend-api-reference"ï¼‰
            standard_patterns = [
                # åŒ¹é… "1-overview", "2-getting-started" ç­‰æ¨¡å¼
                r'"([0-9]{1,2}-[a-zA-Z][a-zA-Z0-9-]*)"',
                r"'([0-9]{1,2}-[a-zA-Z][a-zA-Z0-9-]*)'",
                # åŒ¹é… "1.1-system-architecture", "4.1-backend-api-reference" ç­‰æ¨¡å¼
                r'"([0-9]{1,2}\.[0-9]{1,2}-[a-zA-Z][a-zA-Z0-9-]*)"',
                r"'([0-9]{1,2}\.[0-9]{1,2}-[a-zA-Z][a-zA-Z0-9-]*)'",
                # åŒ¹é…è·¯å¾„ä¸­çš„æ–‡ä»¶å
                r'\/([0-9]{1,2}-[a-zA-Z][a-zA-Z0-9-]*)(?:\/|"|\?|$)',
                r'\/([0-9]{1,2}\.[0-9]{1,2}-[a-zA-Z][a-zA-Z0-9-]*)(?:\/|"|\?|$)',
                # ä¸ç”¨å¼•å·åŒ…å›´çš„æ¨¡å¼
                r'\b([0-9]{1,2}-[a-zA-Z][a-zA-Z0-9-]*)\b',
                r'\b([0-9]{1,2}\.[0-9]{1,2}-[a-zA-Z][a-zA-Z0-9-]*)\b',
            ]
            
            # æ”¶é›†æ‰€æœ‰åŒ¹é…çš„æ ‡å‡†æ–‡ä»¶å
            found_standard_names = []
            for pattern in standard_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    if len(match) >= 5:  # è‡³å°‘åƒ "1-abc" è¿™æ ·çš„é•¿åº¦
                        found_standard_names.append(match)
                        logger.debug(f"ğŸ¯ æ‰¾åˆ°æ ‡å‡†åºå·æ–‡ä»¶å: {match}")
            
            # å¦‚æœæ‰¾åˆ°æ ‡å‡†æ ¼å¼çš„æ–‡ä»¶åï¼Œè¿”å›æœ€åˆé€‚çš„
            if found_standard_names:
                # æŒ‰é•¿åº¦å’Œå¤æ‚åº¦æ’åºï¼Œé€‰æ‹©æœ€åˆç†çš„
                best_name = min(found_standard_names, key=lambda x: (len(x), x))
                logger.debug(f"âœ… é€‰æ‹©æœ€ä½³æ ‡å‡†æ–‡ä»¶å: {best_name}")
                return best_name
            
            # 2. å¦‚æœæ²¡æ‰¾åˆ°æ ‡å‡†æ ¼å¼ï¼Œæœç´¢å¯èƒ½çš„è·¯ç”±æˆ–slugä¿¡æ¯
            route_patterns = [
                # åœ¨Next.jsè·¯ç”±æ•°æ®ä¸­æœç´¢
                r'"pathname":\s*"[^"]*\/([^"\/]+)"',
                r'"href":\s*"[^"]*\/([^"\/]+)"',
                r'"slug":\s*"([^"]+)"',
                # åœ¨Reactç»„ä»¶propsä¸­æœç´¢
                r'"params":\s*{[^}]*"slug":\s*"([^"]*)"',
                r'"query":\s*{[^}]*"slug":\s*"([^"]*)"',
                # åœ¨é¡µé¢å…ƒæ•°æ®ä¸­æœç´¢
                r'"page":\s*{[^}]*"slug":\s*"([^"]*)"',
                r'"route":\s*"[^"]*\/([^"\/]+)"',
            ]
            
            route_candidates = []
            for pattern in route_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    if match and len(match) > 2 and not match.isdigit():
                        # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯æ–‡ä»¶å
                        if re.match(r'^[a-zA-Z0-9][a-zA-Z0-9-_.]*$', match):
                            route_candidates.append(match)
                            logger.debug(f"ğŸ“ å‘ç°è·¯ç”±å€™é€‰: {match}")
            
            # 3. ç‰¹åˆ«æœç´¢å¯èƒ½è¢«ç¼–ç çš„æ–‡ä»¶å
            encoded_patterns = [
                # æœç´¢è¢«HTMLç¼–ç çš„å†…å®¹
                r'&quot;([0-9]{1,2}[-\.][a-zA-Z][a-zA-Z0-9-]*)&quot;',
                # æœç´¢è¢«è½¬ä¹‰çš„JSONå­—ç¬¦ä¸²
                r'\\"([0-9]{1,2}[-\.][a-zA-Z][a-zA-Z0-9-]*)\\"',
                # æœç´¢URLç¼–ç çš„å†…å®¹
                r'%22([0-9]{1,2}[-\.][a-zA-Z][a-zA-Z0-9-]*)%22',
            ]
            
            for pattern in encoded_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    if len(match) >= 5:
                        logger.debug(f"ğŸ”“ æ‰¾åˆ°ç¼–ç çš„æ–‡ä»¶å: {match}")
                        return match
            
            # 4. å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œè¿”å›æœ€ä½³çš„è·¯ç”±å€™é€‰
            if route_candidates:
                # ä¼˜å…ˆé€‰æ‹©åŒ…å«è¿å­—ç¬¦çš„ï¼ˆæ›´å¯èƒ½æ˜¯æ–‡ä»¶åï¼‰
                dash_candidates = [c for c in route_candidates if '-' in c]
                if dash_candidates:
                    best_candidate = min(dash_candidates, key=len)
                    logger.debug(f"ğŸ”„ é€‰æ‹©è¿å­—ç¬¦å€™é€‰: {best_candidate}")
                    return best_candidate
                else:
                    best_candidate = min(route_candidates, key=len)
                    logger.debug(f"ğŸ”„ é€‰æ‹©è·¯ç”±å€™é€‰: {best_candidate}")
                    return best_candidate
                
        except Exception as e:
            logger.debug(f"æå–åŸå§‹æ–‡ä»¶åå¤±è´¥: {e}")
        
        return None
    
    def _sort_pages_by_order(self, pages: list) -> list:
        """æŒ‰ç…§åºå·å’Œæ ‡é¢˜å¯¹é¡µé¢è¿›è¡Œæ’åº"""
        def get_sort_key(page):
            slug = page.get('slug', '')
            title = page.get('title', '')
            
            # å°è¯•ä» slug ä¸­æå–åºå·
            slug_match = re.match(r'^(\d+)', slug)
            if slug_match:
                return (int(slug_match.group(1)), slug)
            
            # å°è¯•ä»æ ‡é¢˜ä¸­æå–åºå·
            title_match = re.match(r'^(\d+)', title)
            if title_match:
                return (int(title_match.group(1)), title)
            
            # ä½¿ç”¨ order å±æ€§ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            order = page.get('order', 9999)
            if order != 9999:
                return (order, title)
            
            # é»˜è®¤æŒ‰æ ‡é¢˜æ’åºï¼Œä½†æ”¾åœ¨æœ€å
            return (9999, title)
        
        return sorted(pages, key=get_sort_key)
    
    def _clean_merged_content(self, content: str) -> str:
        """æ¸…ç†åˆå¹¶åçš„å†…å®¹"""
        # ç§»é™¤é‡å¤çš„æ ‡é¢˜
        lines = content.split('\n')
        cleaned_lines = []
        seen_title = None
        
        for line in lines:
            line_stripped = line.strip()
            
            # å¦‚æœæ˜¯æ ‡é¢˜è¡Œ
            if line_stripped.startswith('# ') and not line_stripped.startswith('# #'):
                title = line_stripped.lstrip('#').strip()
                if seen_title is None:
                    seen_title = title
                    cleaned_lines.append(line)
                elif title == seen_title:
                    # è·³è¿‡é‡å¤çš„æ ‡é¢˜
                    continue
                else:
                    # æ–°çš„æ ‡é¢˜ï¼Œå¯èƒ½æ˜¯å­ç« èŠ‚
                    cleaned_lines.append(line)
            else:
                cleaned_lines.append(line)
        
        # ç§»é™¤æ˜æ˜¾çš„æ•°æ®è¡Œ
        final_lines = []
        in_code_block = False
        
        for line in cleaned_lines:
            line_stripped = line.strip()
            
            # å¤„ç†ä»£ç å—
            if line_stripped.startswith('```'):
                in_code_block = not in_code_block
                final_lines.append(line)
                continue
            
            # åœ¨ä»£ç å—å†…ï¼Œä¿ç•™æ‰€æœ‰å†…å®¹
            if in_code_block:
                final_lines.append(line)
                continue
            
            # è·³è¿‡æ˜æ˜¾çš„æ•°æ®è¡Œï¼Œä½†ä¿ç•™å…¶ä»–å†…å®¹
            if (line_stripped.startswith('{"') or 
                line_stripped.startswith('["') or
                ('"ID":' in line_stripped and '"' in line_stripped) or
                (line_stripped.count('"') > 6 and ':' in line_stripped)):
                continue
            
            final_lines.append(line)
        
        return '\n'.join(final_lines).strip()
    
    def _generate_slug(self, title: str, original_filename: str = None) -> str:
        """ç”Ÿæˆé¡µé¢çš„ slugï¼Œä¼˜å…ˆä½¿ç”¨æ­£ç¡®çš„åŸå§‹æ–‡ä»¶å"""
        
        # å¦‚æœæœ‰æ­£ç¡®çš„åŸå§‹æ–‡ä»¶åï¼Œç›´æ¥ä½¿ç”¨
        if original_filename:
            logger.debug(f"âœ… ä½¿ç”¨åŸå§‹æ–‡ä»¶å: {title} -> {original_filename}")
            return original_filename
        
        # å¦‚æœæ²¡æœ‰æ˜ å°„çš„æ–‡ä»¶åï¼Œæ£€æŸ¥æ ‡é¢˜æ˜¯å¦ä»¥åºå·å¼€å¤´
        title_with_number = re.match(r'^(\d+)[\.\s-]*(.+)', title)
        if title_with_number:
            number = title_with_number.group(1)
            clean_title = title_with_number.group(2)
            # ç”Ÿæˆå¸¦åºå·çš„ slug
            slug = re.sub(r'[^\w\s-]', '', clean_title.lower())
            slug = re.sub(r'[\s_-]+', '-', slug)
            slug = slug.strip('-')
            final_slug = f"{number}-{slug}" if slug else number
            logger.debug(f"ğŸ”¢ ä»æ ‡é¢˜æå–åºå·: {title} -> {final_slug}")
            return final_slug
        
        # é»˜è®¤å¤„ç†ï¼šè½¬æ¢ä¸ºå°å†™ï¼Œæ›¿æ¢ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        slug = re.sub(r'[^\w\s-]', '', title.lower())
        slug = re.sub(r'[\s_-]+', '-', slug)
        final_slug = slug.strip('-')
        logger.debug(f"ğŸ“ ç”Ÿæˆæ ‡å‡†slug: {title} -> {final_slug}")
        return final_slug
    
    def _extract_page_info(self, html_content: str, url: str) -> dict:
        """ä»é¡µé¢æå–ä¿¡æ¯"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æå–é¡µé¢æ ‡é¢˜
        title = "DeepWiki æ–‡æ¡£"
        
        # å°è¯•å¤šç§æ–¹å¼è·å–æ ‡é¢˜
        title_candidates = [
            soup.find('title'),
            soup.find('h1'),
            soup.find('h2'),
            soup.select_one('[data-testid="page-title"]'),
            soup.select_one('.page-title'),
            soup.select_one('.title')
        ]
        
        for candidate in title_candidates:
            if candidate and candidate.get_text().strip():
                title = candidate.get_text().strip()
                # æ¸…ç†æ ‡é¢˜
                title = re.sub(r'\s+', ' ', title)
                title = title.replace(' | DeepWiki', '')
                break
        
        # ä» URL æå–é¡¹ç›®ä¿¡æ¯
        parsed_url = urlparse(url)
        path_parts = [part for part in parsed_url.path.strip('/').split('/') if part]
        
        project_name = ""
        if path_parts:
            if len(path_parts) >= 2:
                project_name = f"{path_parts[0]}/{path_parts[1]}"
            else:
                project_name = path_parts[0]
        
        return {
            'title': title,
            'project_name': project_name,
            'url': url,
            'path_parts': path_parts,
            'github_info': self._extract_github_info(html_content, project_name)
        }
    
    def _extract_github_info(self, html_content: str, project_name: str) -> dict:
        """ä»é¡µé¢ä¸­æå– GitHub ä»“åº“å’Œ commit ä¿¡æ¯"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        github_repo = ""
        commit_sha = ""
        
        # æŸ¥æ‰¾ GitHub é“¾æ¥çš„å¤šç§æ–¹å¼
        github_patterns = [
            # ç›´æ¥æŸ¥æ‰¾ GitHub é“¾æ¥
            r'https://github\.com/([^/]+/[^/\s"\'<>]+)',
            # ä»è„šæœ¬ä¸­æŸ¥æ‰¾
            r'github\.com/([^/]+/[^/\s"\'<>]+)',
            # ä»æºç é“¾æ¥ä¸­æŸ¥æ‰¾
            r'source.*?github\.com/([^/]+/[^/\s"\'<>]+)',
        ]
        
        # åœ¨é¡µé¢æ–‡æœ¬ä¸­æœç´¢ä»“åº“
        for pattern in github_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            if matches:
                for match in matches:
                    # æ¸…ç†åŒ¹é…ç»“æœ
                    repo = match.strip().rstrip('/')
                    if '/' in repo and not repo.endswith('.git'):
                        github_repo = f"https://github.com/{repo}"
                        logger.info(f"ğŸ”— å‘ç° GitHub ä»“åº“: {github_repo}")
                        break
                if github_repo:
                    break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä» project_name æ„é€ 
        if not github_repo and project_name and '/' in project_name:
            github_repo = f"https://github.com/{project_name}"
            logger.info(f"ğŸ“ æ¨æµ‹ GitHub ä»“åº“: {github_repo}")
        
        # æŸ¥æ‰¾ commit SHA
        commit_patterns = [
            # æŸ¥æ‰¾ commit hash æ¨¡å¼ï¼ˆ7-40å­—ç¬¦çš„åå…­è¿›åˆ¶ï¼‰
            r'commit[/_:\s]+([a-f0-9]{7,40})',
            r'sha[/_:\s]+([a-f0-9]{7,40})',
            r'revision[/_:\s]+([a-f0-9]{7,40})',
            # æŸ¥æ‰¾ blob é“¾æ¥ä¸­çš„ commit
            r'github\.com/[^/]+/[^/]+/blob/([a-f0-9]{7,40})',
            # åœ¨è„šæœ¬æ•°æ®ä¸­æŸ¥æ‰¾
            r'"commit"[^"]*"([a-f0-9]{7,40})"',
            r'"sha"[^"]*"([a-f0-9]{7,40})"',
            # åœ¨ meta æ ‡ç­¾ä¸­æŸ¥æ‰¾
            r'<meta[^>]*content="[^"]*([a-f0-9]{7,40})[^"]*"',
        ]
        
        for pattern in commit_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            if matches:
                # è¿‡æ»¤å‡ºåˆæ³•çš„ commit SHAï¼ˆè‡³å°‘7ä½ï¼Œæœ€å¤š40ä½ï¼‰
                for match in matches:
                    if 7 <= len(match) <= 40:
                        commit_sha = match[:8]  # ä½¿ç”¨å‰8ä½
                        logger.info(f"ğŸ¯ å‘ç° commit SHA: {commit_sha}")
                        break
                if commit_sha:
                    break
        
        if not commit_sha:
            logger.warning("âš ï¸ æœªæ‰¾åˆ° commit SHAï¼Œå°†ä½¿ç”¨ main åˆ†æ”¯")
            commit_sha = "main"
        
        if not github_repo:
            logger.warning("âš ï¸ æœªæ‰¾åˆ° GitHub ä»“åº“ä¿¡æ¯")
        
        return {
            'repo_url': github_repo,
            'commit_sha': commit_sha
        }
    
    def convert(self) -> dict:
        """æ‰§è¡Œè½¬æ¢"""
        logger.info(f"ğŸš€ å¼€å§‹è½¬æ¢ DeepWiki ç«™ç‚¹: {self.base_url}")
        
        try:
            # è·å–ä¸»é¡µå†…å®¹
            html_content = self._get_page_content(self.base_url)
            if not html_content:
                raise Exception("æ— æ³•è·å–é¡µé¢å†…å®¹")
            
            # æå–åŸºæœ¬é¡µé¢ä¿¡æ¯
            main_page_info = self._extract_page_info(html_content, self.base_url)
            
            # å¦‚æœä½¿ç”¨Seleniumï¼Œå°è¯•æå–åŠ¨æ€å¯¼èˆªæ•°æ®
            dynamic_navigation = {}
            if self.use_selenium and self.driver:
                logger.info("ğŸ¯ æå–åŠ¨æ€å¯¼èˆªæ•°æ®...")
                dynamic_navigation = self._extract_dynamic_navigation_data(html_content)
            
            # ä»å¯¼èˆªæ æå–çœŸå®çš„æ–‡ä»¶åæ˜ å°„
            logger.info("ğŸ”— åˆ†æå¯¼èˆªæ é“¾æ¥...")
            navigation_links = self._extract_navigation_links(html_content)
            
            # åˆå¹¶åŠ¨æ€å’Œé™æ€å¯¼èˆªæ•°æ®
            if dynamic_navigation:
                logger.info(f"ğŸ”„ åˆå¹¶ {len(dynamic_navigation)} ä¸ªåŠ¨æ€å¯¼èˆªé¡¹...")
                navigation_links.update(dynamic_navigation)
            
            # æå–æ‰€æœ‰é¡µé¢å†…å®¹
            logger.info("ğŸ” æå–é¡µé¢å†…å®¹...")
            pages, navigation_structure = self._extract_nextjs_content(html_content, navigation_links)
            
            if pages:
                logger.info(f"ğŸ“‹ å‘ç° {len(pages)} ä¸ªé¡µé¢")
                for page in pages:
                    logger.info(f"  - {page['title']}")
            else:
                logger.warning("æœªå‘ç°å…¶ä»–é¡µé¢ï¼Œå°†åˆ›å»ºé»˜è®¤é¡µé¢")
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é¡µé¢ï¼Œåˆ›å»ºé»˜è®¤é¡µé¢
                pages = [{
                    'title': main_page_info['project_name'],
                    'content': f"# {main_page_info['project_name']}\n\n> è¿™æ˜¯é»˜è®¤é¡µé¢å†…å®¹",
                    'slug': 'home'
                }]
            
            # å¯¹é¡µé¢è¿›è¡Œæ’åº
            pages = self._sort_pages_by_order(pages)
            
            # åˆ›å»ºé¡µé¢æ–‡ä»¶
            self._create_page_files(pages, main_page_info)
            
            # ç”Ÿæˆ Docsify é…ç½®æ–‡ä»¶
            self._generate_docsify_files(main_page_info, pages, navigation_structure)
            
            return {
                'success': True,
                'pages_processed': len(pages),
                'assets_downloaded': len(self.downloaded_assets),
                'output_dir': str(self.output_dir.absolute()),
                'main_page': main_page_info,
                'pages': pages
            }
            
        except Exception as e:
            logger.error(f"è½¬æ¢å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
        finally:
            if self.driver:
                self.driver.quit()
                logger.info("ğŸ›‘ Selenium WebDriver å·²å…³é—­")
    
    def _create_page_files(self, pages: list, main_page_info: dict):
        """åˆ›å»ºé¡µé¢æ–‡ä»¶"""
        
        # è·å–é¡¹ç›®è·¯å¾„ä¿¡æ¯
        path_parts = main_page_info['path_parts']
        
        # ä¿å­˜è·¯å¾„ä¿¡æ¯ä¾›å…¶ä»–æ–¹æ³•ä½¿ç”¨
        self.processed_path_parts = path_parts
        
        if self.multilingual:
            # å¤šè¯­è¨€ç‰ˆæœ¬ï¼šä¸ºæ¯ç§è¯­è¨€åˆ›å»ºæ–‡ä»¶
            for lang_code in ['zh-cn', 'en']:
                if path_parts and len(path_parts) >= 2:
                    base_dir = self.output_dir / lang_code / "pages" / path_parts[0] / path_parts[1]
                else:
                    base_dir = self.output_dir / lang_code / "pages"
                
                base_dir.mkdir(parents=True, exist_ok=True)
                
                # åˆ›å»ºæ¯ä¸ªé¡µé¢æ–‡ä»¶
                for page in pages:
                    filename = f"{page['slug']}.md"
                    file_path = base_dir / filename
                    
                    # å¤„ç† Sources é“¾æ¥
                    processed_content = self._process_sources_links(page['content'], main_page_info.get('github_info', {}))
                    
                    # å†™å…¥é¡µé¢å†…å®¹
                    file_path.write_text(processed_content, encoding='utf-8')
                    logger.info(f"ğŸ“„ åˆ›å»ºé¡µé¢: {page['title']} -> {file_path.relative_to(self.output_dir)}")
        else:
            # å•è¯­è¨€ç‰ˆæœ¬
            # åˆ›å»ºé¡µé¢ç›®å½•
            if path_parts and len(path_parts) >= 2:
                base_dir = self.output_dir / "pages" / path_parts[0] / path_parts[1]
            else:
                base_dir = self.output_dir / "pages"
            
            base_dir.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»ºæ¯ä¸ªé¡µé¢æ–‡ä»¶
            created_files = {}  # ç”¨äºè·Ÿè¸ªå·²åˆ›å»ºçš„æ–‡ä»¶å
            for page in pages:
                base_slug = page['slug']
                slug = base_slug
                counter = 1
                
                # ç¡®ä¿æ–‡ä»¶åå”¯ä¸€æ€§
                while slug in created_files:
                    slug = f"{base_slug}-{counter}"
                    counter += 1
                
                filename = f"{slug}.md"
                file_path = base_dir / filename
                created_files[slug] = True
                
                # æ›´æ–°é¡µé¢çš„ slugï¼ˆç”¨äºä¾§è¾¹æ ç”Ÿæˆï¼‰
                page['slug'] = slug
            
                # å¤„ç† Sources é“¾æ¥
                processed_content = self._process_sources_links(page['content'], main_page_info.get('github_info', {}))
                
                # å†™å…¥é¡µé¢å†…å®¹
                file_path.write_text(processed_content, encoding='utf-8')
                
                # è®°å½•å¤„ç†çš„é¡µé¢
                relative_path = str(file_path.relative_to(self.output_dir))
                self.processed_pages.append({
                    'file': relative_path,
                    'title': page['title'],
                    'slug': page['slug']
                })
                
                logger.info(f"ğŸ“„ åˆ›å»ºé¡µé¢: {page['title']} -> {relative_path}")
    
    def _generate_docsify_files(self, main_page_info: dict, pages: list = None, navigation_structure: list = None):
        """ç”Ÿæˆ Docsify é…ç½®æ–‡ä»¶"""
        
        if self.multilingual:
            # å¤šè¯­è¨€ç‰ˆæœ¬
            self._generate_multilingual_files(main_page_info, pages or [], navigation_structure)
        else:
            # å•è¯­è¨€ç‰ˆæœ¬
            # ç”Ÿæˆä¾§è¾¹æ 
            self._generate_sidebar_with_pages(pages or [], navigation_structure)
            
            # ç”Ÿæˆ index.html
            self._generate_index_html(main_page_info['project_name'])
            
            # ç”Ÿæˆä¸» README.md
            self._generate_main_readme_with_pages(main_page_info, pages or [])
        
        # ç”Ÿæˆ .nojekyll æ–‡ä»¶
        (self.output_dir / ".nojekyll").write_text("", encoding='utf-8')
    
    def _generate_multilingual_files(self, main_page_info: dict, pages: list, navigation_structure: list = None):
        """ç”Ÿæˆå¤šè¯­è¨€ç‰ˆæœ¬çš„é…ç½®æ–‡ä»¶"""
        project_name = main_page_info['project_name']
        
        # 1. ç”Ÿæˆæ ¹ç›®å½•çš„ index.htmlï¼ˆå¤šè¯­è¨€ç‰ˆæœ¬ï¼‰
        self._generate_multilingual_index_html(project_name)
        
        # 2. ç”Ÿæˆæ ¹ç›®å½•çš„ä¾§è¾¹æ ï¼ˆè¯­è¨€é€‰æ‹©ï¼‰
        self._generate_root_sidebar()
        
        # 3. ç”Ÿæˆæ ¹ç›®å½•çš„ README.md
        self._generate_language_selection_readme(main_page_info)
        
        # 4. ç”Ÿæˆä¸­æ–‡ç‰ˆæœ¬
        self._generate_language_version('zh-cn', main_page_info, pages, 'ä¸­æ–‡', navigation_structure)
        
        # 5. ç”Ÿæˆè‹±æ–‡ç‰ˆæœ¬
        self._generate_language_version('en', main_page_info, pages, 'English', navigation_structure)
        
        logger.info("ğŸŒ å¤šè¯­è¨€æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
    
    def _generate_multilingual_index_html(self, site_name: str):
        """ç”Ÿæˆå¤šè¯­è¨€ç‰ˆæœ¬çš„ index.html"""
        html_content = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>{site_name}</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="description" content="ä» DeepWiki è½¬æ¢çš„æ–‡æ¡£ç«™ç‚¹">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <link rel="stylesheet" href="//unpkg.com/docsify@4/lib/themes/vue.css">
  <style>
    .sidebar {{
      padding-top: 6px;
    }}
    .markdown-section {{
      max-width: 800px;
    }}
    .app-name-link {{
      color: var(--theme-color, #42b983) !important;
    }}
    /* è¯­è¨€åˆ‡æ¢æŒ‰é’®æ ·å¼ */
    .language-switch {{
      position: fixed;
      top: 20px;
      right: 20px;
      z-index: 1000;
      background: var(--theme-color, #42b983);
      color: white;
      border: none;
      border-radius: 4px;
      padding: 8px 12px;
      cursor: pointer;
      font-size: 14px;
      text-decoration: none;
      transition: background-color 0.3s;
    }}
    .language-switch:hover {{
      background: var(--theme-color-dark, #369870);
      color: white;
    }}
    .language-menu {{
      position: fixed;
      top: 60px;
      right: 20px;
      z-index: 1000;
      background: white;
      border: 1px solid #eee;
      border-radius: 4px;
      box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      display: none;
      min-width: 120px;
    }}
    .language-menu a {{
      display: block;
      padding: 10px 15px;
      color: #333;
      text-decoration: none;
      border-bottom: 1px solid #eee;
    }}
    .language-menu a:last-child {{
      border-bottom: none;
    }}
    .language-menu a:hover {{
      background: #f8f9fa;
    }}
    .language-menu.show {{
      display: block;
    }}
  </style>
</head>
<body>
  <div id="app">æ­£åœ¨åŠ è½½...</div>
  
  <!-- è¯­è¨€åˆ‡æ¢èœå• -->
  <button class="language-switch" onclick="toggleLanguageMenu()">
    ğŸŒ è¯­è¨€ / Language
  </button>
  <div class="language-menu" id="languageMenu">
    <a href="#/zh-cn/">ğŸ‡¨ğŸ‡³ ä¸­æ–‡</a>
    <a href="#/en/">ğŸ‡ºğŸ‡¸ English</a>
  </div>

  <script>
    // è¯­è¨€åˆ‡æ¢åŠŸèƒ½
    function toggleLanguageMenu() {{
      const menu = document.getElementById('languageMenu');
      menu.classList.toggle('show');
    }}
    
    // ç‚¹å‡»é¡µé¢å…¶ä»–åœ°æ–¹å…³é—­èœå•
    document.addEventListener('click', function(event) {{
      const menu = document.getElementById('languageMenu');
      const button = document.querySelector('.language-switch');
      if (!menu.contains(event.target) && !button.contains(event.target)) {{
        menu.classList.remove('show');
      }}
    }});

    window.$docsify = {{
      name: '{site_name}',
      repo: '',
      homepage: 'zh-cn/README.md',
      loadSidebar: '_sidebar.md',
      autoHeader: true,
      subMaxLevel: 3,
      maxLevel: 4,
      alias: {{
        '.*zh-cn.*/_sidebar.md': '/zh-cn/_sidebar.md',
        '.*en.*/_sidebar.md': '/en/_sidebar.md',
        '/zh-cn/README.md': '/zh-cn/README.md',
        '/en/README.md': '/en/README.md',
        '/zh-cn/pages/(.*)': '/zh-cn/pages/$1',
        '/en/pages/(.*)': '/en/pages/$1'
      }},
      fallbackLanguages: ['zh-cn'],
      nameLink: {{
        '/zh-cn/': '#/zh-cn/',
        '/en/': '#/en/',
        '/': '#/'
      }},
      search: {{
        maxAge: 86400000,
        paths: 'auto',
        placeholder: {{
          '/zh-cn/': 'æœç´¢æ–‡æ¡£...',
          '/en/': 'Search...',
          '/': 'æœç´¢æ–‡æ¡£...'
        }},
        noData: {{
          '/zh-cn/': 'æ²¡æœ‰æ‰¾åˆ°ç»“æœ',
          '/en/': 'No results found',
          '/': 'æ²¡æœ‰æ‰¾åˆ°ç»“æœ'
        }},
        depth: 6
      }},
      copyCode: {{
        buttonText: {{
          '/zh-cn/': 'å¤åˆ¶ä»£ç ',
          '/en/': 'Copy Code',
          '/': 'å¤åˆ¶ä»£ç '
        }},
        errorText: {{
          '/zh-cn/': 'å¤åˆ¶å¤±è´¥',
          '/en/': 'Copy failed',
          '/': 'å¤åˆ¶å¤±è´¥'
        }},
        successText: {{
          '/zh-cn/': 'å·²å¤åˆ¶åˆ°å‰ªè´´æ¿',
          '/en/': 'Copied to clipboard',
          '/': 'å·²å¤åˆ¶åˆ°å‰ªè´´æ¿'
        }}
      }},
      pagination: {{
        previousText: {{
          '/zh-cn/': 'ä¸Šä¸€é¡µ',
          '/en/': 'Previous',
          '/': 'ä¸Šä¸€é¡µ'
        }},
        nextText: {{
          '/zh-cn/': 'ä¸‹ä¸€é¡µ',
          '/en/': 'Next',
          '/': 'ä¸‹ä¸€é¡µ'
        }},
        crossChapter: true,
        crossChapterText: true
      }},
      mermaid: {{
        theme: 'default'
      }}
    }}
  </script>
  <!-- Docsify v4 -->
  <script src="//unpkg.com/docsify@4"></script>
  <!-- Mermaid -->
  <script src="//unpkg.com/mermaid@9/dist/mermaid.min.js"></script>
  <script>
    mermaid.initialize({{
      theme: 'default',
      startOnLoad: false
    }});
  </script>
  <script src="//unpkg.com/docsify-mermaid@1/dist/docsify-mermaid.js"></script>
  <!-- æ’ä»¶ -->
  <script src="//unpkg.com/docsify/lib/plugins/search.min.js"></script>
  <script src="//unpkg.com/docsify/lib/plugins/zoom-image.min.js"></script>
  <script src="//unpkg.com/docsify-copy-code@2"></script>
  <script src="//unpkg.com/docsify-pagination@2/dist/docsify-pagination.min.js"></script>
</body>
</html>'''
        
        index_file = self.output_dir / "index.html"
        index_file.write_text(html_content, encoding='utf-8')
        logger.info("ğŸŒ ç”Ÿæˆå¤šè¯­è¨€ index.html...")
    
    def _generate_root_sidebar(self):
        """ç”Ÿæˆæ ¹ç›®å½•çš„ä¾§è¾¹æ ï¼ˆè¯­è¨€é€‰æ‹©ï¼‰"""
        sidebar_content = '''<!-- å¤šè¯­è¨€æ–‡æ¡£å¯¼èˆª -->

* [ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ–‡æ¡£](zh-cn/)
* [ğŸ‡ºğŸ‡¸ English Docs](en/)
'''
        
        sidebar_file = self.output_dir / "_sidebar.md"
        sidebar_file.write_text(sidebar_content, encoding='utf-8')
        logger.info("ğŸ“‹ ç”Ÿæˆæ ¹ç›®å½•ä¾§è¾¹æ ...")
    
    def _generate_language_selection_readme(self, main_page_info: dict):
        """ç”Ÿæˆè¯­è¨€é€‰æ‹©é¡µé¢"""
        github_repo_link = ""
        if main_page_info.get('github_info', {}).get('repo_url'):
            github_repo_link = f"""
- **æºç ä»“åº“ / Source Repository**: [{main_page_info['github_info']['repo_url']}]({main_page_info['github_info']['repo_url']})"""

        readme_content = f'''# {main_page_info['project_name']}

> ğŸŒ å¤šè¯­è¨€æ–‡æ¡£ç«™ç‚¹ / Multilingual Documentation Site

## è¯­è¨€é€‰æ‹© / Language Selection

è¯·é€‰æ‹©æ‚¨çš„è¯­è¨€ / Please select your language:

### [ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ–‡æ¡£](zh-cn/)
- å®Œæ•´çš„ä¸­æ–‡æ–‡æ¡£
- ä¸­æ–‡ç•Œé¢å’Œæœç´¢
- ä¸­æ–‡å¯¼èˆªèœå•

### [ğŸ‡ºğŸ‡¸ English Documentation](en/)
- Complete English documentation  
- English interface and search
- English navigation menu

---

## é¡¹ç›®ä¿¡æ¯ / Project Information

- **é¡¹ç›®åç§° / Project Name**: {main_page_info['project_name']}
- **åŸå§‹é¡µé¢ / Original Page**: [{main_page_info['url']}]({main_page_info['url']}){github_repo_link}
- **ç”Ÿæˆæ—¶é—´ / Generated**: {time.strftime('%Y-%m-%d %H:%M:%S')}

## æŠ€æœ¯æ”¯æŒ / Technical Support

æ­¤æ–‡æ¡£ç«™ç‚¹ä½¿ç”¨ [Docsify](https://docsify.js.org/) æ„å»º / This documentation site is built with [Docsify](https://docsify.js.org/)

---

*ç”± DeepWiki2Docsify å·¥å…·è‡ªåŠ¨ç”Ÿæˆ / Automatically generated by DeepWiki2Docsify tool*
'''
        
        readme_file = self.output_dir / "README.md"
        readme_file.write_text(readme_content, encoding='utf-8')
        logger.info("ğŸ“„ ç”Ÿæˆè¯­è¨€é€‰æ‹©é¡µé¢...")
    
    def _generate_language_version(self, lang_code: str, main_page_info: dict, pages: list, lang_name: str, navigation_structure: list = None):
        """ç”Ÿæˆç‰¹å®šè¯­è¨€ç‰ˆæœ¬çš„é…ç½®æ–‡ä»¶"""
        lang_dir = self.output_dir / lang_code
        
        # ç”Ÿæˆè¯¥è¯­è¨€çš„ä¾§è¾¹æ 
        self._generate_language_sidebar(lang_dir, pages, lang_code, navigation_structure)
        
        # ç”Ÿæˆè¯¥è¯­è¨€çš„ README.md
        self._generate_language_readme(lang_dir, main_page_info, pages, lang_code, lang_name)
        
        logger.info(f"ğŸ“ ç”Ÿæˆ {lang_name} ç‰ˆæœ¬é…ç½®...")
    
    def _generate_language_sidebar(self, lang_dir: Path, pages: list, lang_code: str, navigation_structure: list = None):
        """ç”Ÿæˆç‰¹å®šè¯­è¨€çš„ä¾§è¾¹æ """
        if lang_code == 'zh-cn':
            sidebar_content = "<!-- ä¸­æ–‡æ–‡æ¡£å¯¼èˆª -->\n\n* [é¦–é¡µ](zh-cn/README.md)\n\n"
        else:  # en
            sidebar_content = "<!-- English Documentation Navigation -->\n\n* [Home](en/README.md)\n\n"
        
        if pages:
            # åŠ¨æ€ç”Ÿæˆé¡µé¢è·¯å¾„ï¼ˆæ ¹æ®å®é™…åˆ›å»ºçš„ç›®å½•ç»“æ„ï¼‰
            # ä»ç¬¬ä¸€ä¸ªé¡µé¢çš„å¤„ç†ä¿¡æ¯ä¸­è·å–è·¯å¾„ç»“æ„
            path_parts = []
            if hasattr(self, 'processed_path_parts'):
                path_parts = self.processed_path_parts
            
            # æ„å»ºè·¯å¾„å‰ç¼€
            if path_parts and len(path_parts) >= 2:
                path_prefix = f"{lang_code}/pages/{path_parts[0]}/{path_parts[1]}"
            else:
                path_prefix = f"{lang_code}/pages"
            
            # ç®€å•æŒ‰å­—æ¯é¡ºåºåˆ—å‡ºæ‰€æœ‰é¡µé¢
            # ä½¿ç”¨ä¸“é—¨çš„å¤šè¯­è¨€å±‚çº§åŒ–ç»„ç»‡æ–¹æ³•
            organized_pages = self._organize_pages_hierarchically_for_multilingual(pages, path_prefix)
            sidebar_content += self._generate_hierarchical_sidebar_content_for_multilingual(organized_pages)
        
        sidebar_file = lang_dir / "_sidebar.md"
        sidebar_file.write_text(sidebar_content, encoding='utf-8')
    
    def _organize_pages_hierarchically_for_multilingual(self, pages: list, path_prefix: str) -> dict:
        """ä¸ºå¤šè¯­è¨€æ¨¡å¼æŒ‰ç…§æ–‡ä»¶ååºå·ç»„ç»‡é¡µé¢å±‚çº§ç»“æ„"""
        organized = {}
        
        for page in pages:
            title = page['title']
            slug = page['slug']
            relative_path = f"{path_prefix}/{slug}.md"
            
            # è§£ææ–‡ä»¶ååºå·
            sequence_info = self._parse_filename_sequence(slug)
            if sequence_info:
                major = sequence_info['major']
                minor = sequence_info.get('minor')
                
                # ç¡®ä¿ä¸»è¦åˆ†ç»„å­˜åœ¨
                if major not in organized:
                    organized[major] = {
                        'main_page': None,
                        'sub_pages': {},
                        'order': major
                    }
                
                if minor is None:
                    # ä¸»é¡µé¢ï¼ˆå¦‚ 1-overview, 2-getting-startedï¼‰
                    organized[major]['main_page'] = {
                        'title': title,
                        'path': relative_path,
                        'slug': slug,
                        'order': major
                    }
                else:
                    # å­é¡µé¢ï¼ˆå¦‚ 1.1-system-architecture, 2.1-installationï¼‰
                    organized[major]['sub_pages'][minor] = {
                        'title': title,
                        'path': relative_path,
                        'slug': slug,
                        'order': minor
                    }
            else:
                # æ²¡æœ‰åºå·çš„é¡µé¢æ”¾åœ¨æœ€å
                if 999 not in organized:
                    organized[999] = {
                        'main_page': None,
                        'sub_pages': {},
                        'order': 999
                    }
                
                # ä½¿ç”¨ä¸€ä¸ªé€’å¢çš„åºå·æ”¾åœ¨å­é¡µé¢ä¸­
                next_order = len(organized[999]['sub_pages']) + 1
                organized[999]['sub_pages'][next_order] = {
                    'title': title,
                    'path': relative_path,
                    'slug': slug,
                    'order': next_order
                }
        
        return organized
    
    def _generate_hierarchical_sidebar_content_for_multilingual(self, organized_pages: dict) -> str:
        """ä¸ºå¤šè¯­è¨€æ¨¡å¼ç”Ÿæˆå±‚çº§åŒ–çš„ä¾§è¾¹æ å†…å®¹"""
        content = ""
        
        # æŒ‰ä¸»è¦åºå·æ’åº
        for major in sorted(organized_pages.keys()):
            group = organized_pages[major]
            
            # å¦‚æœæ˜¯æœ€åçš„æœªåˆ†ç±»ç»„ï¼Œæ·»åŠ æ ‡é¢˜
            if major == 999:
                content += "* ğŸ“‹ å…¶ä»–é¡µé¢\n"
            
            # æ·»åŠ ä¸»é¡µé¢
            if group['main_page']:
                main_page = group['main_page']
                content += f"* [{main_page['title']}]({main_page['path']})\n"
                
                # æ·»åŠ è¯¥ç»„çš„å­é¡µé¢
                if group['sub_pages']:
                    for minor in sorted(group['sub_pages'].keys()):
                        sub_page = group['sub_pages'][minor]
                        content += f"  * [{sub_page['title']}]({sub_page['path']})\n"
            else:
                # å¦‚æœæ²¡æœ‰ä¸»é¡µé¢ï¼Œä½†æœ‰å­é¡µé¢ï¼Œç›´æ¥åˆ—å‡ºå­é¡µé¢
                if group['sub_pages']:
                    if major != 999:  # å¯¹äºæœ‰åºå·ä½†æ²¡æœ‰ä¸»é¡µé¢çš„æƒ…å†µ
                        content += f"* ğŸ“ ç¬¬ {major} éƒ¨åˆ†\n"
                    
                    for minor in sorted(group['sub_pages'].keys()):
                        sub_page = group['sub_pages'][minor]
                        indent = "  " if major != 999 else "  "
                        content += f"{indent}* [{sub_page['title']}]({sub_page['path']})\n"
            
            # åœ¨æ¯ä¸ªä¸»è¦åˆ†ç»„åæ·»åŠ ç©ºè¡Œï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            if major != max(organized_pages.keys()):
                content += "\n"
        
        return content
    
    def _generate_hierarchical_sidebar(self, pages: list, navigation: list, path_prefix: str, lang_code: str) -> str:
        """ç”Ÿæˆå±‚çº§åŒ–çš„ä¾§è¾¹æ """
        sidebar_content = ""
        
        # åˆ›å»ºé¡µé¢æ˜ å°„
        page_map = {page['title']: page for page in pages}
        
        # æŒ‰å¯¼èˆªç»“æ„çš„é¡ºåºå’Œå±‚çº§ç”Ÿæˆä¾§è¾¹æ 
        nav_sorted = sorted(navigation, key=lambda x: (x.get('order', 0), x.get('level', 0)))
        
        for nav_item in nav_sorted:
            title = nav_item['title']
            level = nav_item.get('level', 0)
            
            if title in page_map:
                page = page_map[title]
                relative_path = f"{path_prefix}/{page['slug']}.md"
                
                # æ ¹æ®å±‚çº§æ·»åŠ é€‚å½“çš„ç¼©è¿›
                if level == 0:
                    sidebar_content += f"* [{title}]({relative_path})\n"
                else:
                    indent = "  " * (level + 1)
                    sidebar_content += f"{indent}* [{title}]({relative_path})\n"
        
        # æ·»åŠ æœªåœ¨å¯¼èˆªä¸­çš„é¡µé¢
        if navigation:
            nav_titles = {nav['title'] for nav in navigation}
            unmapped_pages = [page for page in pages if page['title'] not in nav_titles]
            
            if unmapped_pages:
                other_label = "* å…¶ä»–é¡µé¢\n" if lang_code == 'zh-cn' else "* Other Pages\n"
                sidebar_content += other_label
                for page in sorted(unmapped_pages, key=lambda p: p['title']):
                    relative_path = f"{path_prefix}/{page['slug']}.md"
                    sidebar_content += f"  * [{page['title']}]({relative_path})\n"
        
        return sidebar_content
    
    def _generate_language_readme(self, lang_dir: Path, main_page_info: dict, pages: list, lang_code: str, lang_name: str):
        """ç”Ÿæˆç‰¹å®šè¯­è¨€çš„ README.md"""
        
        # æ„å»ºæºç ä»“åº“é“¾æ¥
        github_repo_link = ""
        if main_page_info.get('github_info', {}).get('repo_url'):
            repo_url = main_page_info['github_info']['repo_url']
            if lang_code == 'zh-cn':
                github_repo_link = f"\n- **æºç ä»“åº“**: [{repo_url}]({repo_url})"
            else:
                github_repo_link = f"\n- **Source Repository**: [{repo_url}]({repo_url})"
        
        if lang_code == 'zh-cn':
            readme_content = f'''# {main_page_info['project_name']}

> ğŸš€ ä» DeepWiki è½¬æ¢çš„æ–‡æ¡£ç«™ç‚¹

## ğŸ“– æ–‡æ¡£å¯¼èˆª

æœ¬æ–‡æ¡£åŒ…å« **{len(pages)}** ä¸ªé¡µé¢ï¼Œæ¶µç›–äº†é¡¹ç›®çš„å®Œæ•´æŠ€æœ¯æ–‡æ¡£ã€‚

### æ–‡æ¡£é¡µé¢

è¯·æŸ¥çœ‹å·¦ä¾§å¯¼èˆªæ æµè§ˆæ‰€æœ‰é¡µé¢ï¼Œæˆ–ä½¿ç”¨æœç´¢åŠŸèƒ½å¿«é€Ÿæ‰¾åˆ°æ‰€éœ€å†…å®¹ã€‚

## ğŸ”— å¿«é€Ÿé“¾æ¥

- **åŸå§‹é¡µé¢**: [{main_page_info['url']}]({main_page_info['url']}){github_repo_link}

## ğŸ“ ä½¿ç”¨è¯´æ˜

æ­¤æ–‡æ¡£ç«™ç‚¹æ”¯æŒï¼š

- ğŸ“± å“åº”å¼è®¾è®¡ï¼Œæ”¯æŒç§»åŠ¨ç«¯
- ğŸ” å…¨æ–‡æœç´¢åŠŸèƒ½
- ğŸ–¼ï¸ å›¾ç‰‡ç¼©æ”¾æŸ¥çœ‹
- ğŸ“‹ ä»£ç ä¸€é”®å¤åˆ¶
- ğŸ“„ åˆ†é¡µå¯¼èˆª
- ğŸŒ ä¸­è‹±æ–‡åˆ‡æ¢

---

*ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}*
'''
        else:  # en
            readme_content = f'''# {main_page_info['project_name']}

> ğŸš€ Documentation Site Converted from DeepWiki

## ğŸ“– Documentation Navigation

This documentation contains **{len(pages)}** pages covering the complete technical documentation of the project.

### Documentation Pages

Please browse all pages using the sidebar navigation or use the search function to quickly find the content you need.

## ğŸ”— Quick Links

- **Original Page**: [{main_page_info['url']}]({main_page_info['url']}){github_repo_link}

## ğŸ“ Features

This documentation site supports:

- ğŸ“± Responsive design for mobile devices
- ğŸ” Full-text search functionality
- ğŸ–¼ï¸ Image zoom viewing
- ğŸ“‹ One-click code copying
- ğŸ“„ Page navigation
- ğŸŒ Chinese/English switching

---

*Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}*
'''
        
        readme_file = lang_dir / "README.md"
        readme_file.write_text(readme_content, encoding='utf-8')
    
    def _generate_sidebar_with_pages(self, pages: list, navigation_structure: list = None):
        """ç”ŸæˆåŒ…å«æ‰€æœ‰é¡µé¢çš„å±‚çº§åŒ–ä¾§è¾¹æ """
        sidebar_content = "<!-- docs/_sidebar.md -->\n\n"
        sidebar_content += "* [é¦–é¡µ](README.md)\n\n"
        
        if pages:
            # æŒ‰æ–‡ä»¶ååºå·è¿›è¡Œå±‚çº§æ’åºå’Œåˆ†ç»„
            organized_pages = self._organize_pages_hierarchically(pages)
            sidebar_content += self._generate_hierarchical_sidebar_content(organized_pages)
        
        sidebar_file = self.output_dir / "_sidebar.md"
        sidebar_file.write_text(sidebar_content, encoding='utf-8')
        logger.info("ï¿½ ç”Ÿæˆå±‚çº§åŒ–ä¾§è¾¹æ ...")
    
    def _organize_pages_hierarchically(self, pages: list) -> dict:
        """æŒ‰ç…§æ–‡ä»¶ååºå·ç»„ç»‡é¡µé¢å±‚çº§ç»“æ„"""
        organized = {}
        
        for page in pages:
            # æ‰¾åˆ°è¿™ä¸ªé¡µé¢å¯¹åº”çš„å¤„ç†è®°å½•
            processed_page = None
            for p in self.processed_pages:
                if p['slug'] == page['slug']:
                    processed_page = p
                    break
            
            if not processed_page:
                continue
            
            relative_path = processed_page['file']
            title = page['title']
            slug = page['slug']
            
            # è§£ææ–‡ä»¶ååºå·
            sequence_info = self._parse_filename_sequence(slug)
            if sequence_info:
                major = sequence_info['major']
                minor = sequence_info.get('minor')
                
                # ç¡®ä¿ä¸»è¦åˆ†ç»„å­˜åœ¨
                if major not in organized:
                    organized[major] = {
                        'main_page': None,
                        'sub_pages': {},
                        'order': major
                    }
                
                if minor is None:
                    # ä¸»é¡µé¢ï¼ˆå¦‚ 1-overview, 2-getting-startedï¼‰
                    organized[major]['main_page'] = {
                        'title': title,
                        'path': relative_path,
                        'slug': slug,
                        'order': major
                    }
                else:
                    # å­é¡µé¢ï¼ˆå¦‚ 1.1-system-architecture, 2.1-installationï¼‰
                    organized[major]['sub_pages'][minor] = {
                        'title': title,
                        'path': relative_path,
                        'slug': slug,
                        'order': minor
                    }
            else:
                # æ²¡æœ‰åºå·çš„é¡µé¢æ”¾åœ¨æœ€å
                if 999 not in organized:
                    organized[999] = {
                        'main_page': None,
                        'sub_pages': {},
                        'order': 999
                    }
                
                # ä½¿ç”¨ä¸€ä¸ªé€’å¢çš„åºå·æ”¾åœ¨å­é¡µé¢ä¸­
                next_order = len(organized[999]['sub_pages']) + 1
                organized[999]['sub_pages'][next_order] = {
                    'title': title,
                    'path': relative_path,
                    'slug': slug,
                    'order': next_order
                }
        
        return organized
    
    def _parse_filename_sequence(self, filename: str) -> dict:
        """è§£ææ–‡ä»¶åä¸­çš„åºå·ä¿¡æ¯"""
        # åŒ¹é… "1-overview", "1.1-system-architecture" ç­‰æ ¼å¼
        pattern = r'^(\d+)(?:\.(\d+))?-'
        match = re.match(pattern, filename)
        
        if match:
            major = int(match.group(1))
            minor = int(match.group(2)) if match.group(2) else None
            return {
                'major': major,
                'minor': minor
            }
        
        return None
    
    def _generate_hierarchical_sidebar_content(self, organized_pages: dict) -> str:
        """ç”Ÿæˆå±‚çº§åŒ–çš„ä¾§è¾¹æ å†…å®¹"""
        content = ""
        
        # æŒ‰ä¸»è¦åºå·æ’åº
        for major in sorted(organized_pages.keys()):
            group = organized_pages[major]
            
            # å¦‚æœæ˜¯æœ€åçš„æœªåˆ†ç±»ç»„ï¼Œæ·»åŠ æ ‡é¢˜
            if major == 999:
                content += "* ğŸ“‹ å…¶ä»–é¡µé¢\n"
            
            # æ·»åŠ ä¸»é¡µé¢
            if group['main_page']:
                main_page = group['main_page']
                content += f"* [{main_page['title']}]({main_page['path']})\n"
                
                # æ·»åŠ è¯¥ç»„çš„å­é¡µé¢
                if group['sub_pages']:
                    for minor in sorted(group['sub_pages'].keys()):
                        sub_page = group['sub_pages'][minor]
                        content += f"  * [{sub_page['title']}]({sub_page['path']})\n"
            else:
                # å¦‚æœæ²¡æœ‰ä¸»é¡µé¢ï¼Œä½†æœ‰å­é¡µé¢ï¼Œç›´æ¥åˆ—å‡ºå­é¡µé¢
                if group['sub_pages']:
                    if major != 999:  # å¯¹äºæœ‰åºå·ä½†æ²¡æœ‰ä¸»é¡µé¢çš„æƒ…å†µ
                        content += f"* ğŸ“ ç¬¬ {major} éƒ¨åˆ†\n"
                    
                    for minor in sorted(group['sub_pages'].keys()):
                        sub_page = group['sub_pages'][minor]
                        indent = "  " if major != 999 else "  "
                        content += f"{indent}* [{sub_page['title']}]({sub_page['path']})\n"
            
            # åœ¨æ¯ä¸ªä¸»è¦åˆ†ç»„åæ·»åŠ ç©ºè¡Œï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            if major != max(organized_pages.keys()):
                content += "\n"
        
        return content
    
    def _generate_main_readme_with_pages(self, main_page_info: dict, pages: list):
        """ç”ŸæˆåŒ…å«é¡µé¢å¯¼èˆªçš„ä¸» README.md"""
        
        readme_content = f"""# {main_page_info['project_name']}

> ğŸš€ ä» DeepWiki è½¬æ¢çš„æ–‡æ¡£ç«™ç‚¹

## ğŸ“– æ–‡æ¡£å¯¼èˆª

"""
        
        if pages:
            readme_content += f"æœ¬æ–‡æ¡£åŒ…å« **{len(pages)}** ä¸ªé¡µé¢ï¼š\n\n"
            
            # ç®€å•æŒ‰å­—æ¯é¡ºåºåˆ—å‡ºæ‰€æœ‰é¡µé¢
            readme_content += "### ğŸ“š æ‰€æœ‰é¡µé¢\n\n"
            for page in sorted(pages, key=lambda p: p['title']):
                relative_path = None
                for processed_page in self.processed_pages:
                    if processed_page['slug'] == page['slug']:
                        relative_path = processed_page['file']
                        break
                
                if relative_path:
                    readme_content += f"- [{page['title']}]({relative_path})\n"
            readme_content += "\n"
        else:
            readme_content += "æš‚æ— é¡µé¢å†…å®¹ã€‚\n\n"
        
        readme_content += f"""

## ğŸ”— åŸå§‹é“¾æ¥

- **DeepWiki åŸå§‹é¡µé¢**: [{main_page_info['url']}]({main_page_info['url']})"""
        
        # æ·»åŠ æºç ä»“åº“é“¾æ¥
        if main_page_info.get('github_info', {}).get('repo_url'):
            repo_url = main_page_info['github_info']['repo_url']
            readme_content += f"""
- **æºç ä»“åº“**: [{repo_url}]({repo_url})"""
        
        readme_content += f"""

## ğŸ“ ä½¿ç”¨è¯´æ˜

æ­¤æ–‡æ¡£ç«™ç‚¹ä½¿ç”¨ [Docsify](https://docsify.js.org/) æ„å»ºï¼Œæ”¯æŒï¼š

- ğŸ“± å“åº”å¼è®¾è®¡ï¼Œæ”¯æŒç§»åŠ¨ç«¯
- ğŸ” å…¨æ–‡æœç´¢åŠŸèƒ½
- ğŸ–¼ï¸ å›¾ç‰‡ç¼©æ”¾æŸ¥çœ‹
- ğŸ“‹ ä»£ç ä¸€é”®å¤åˆ¶
- ğŸ“„ åˆ†é¡µå¯¼èˆª
- ğŸ“Š é˜…è¯»è¿›åº¦æ˜¾ç¤º

## ğŸš€ æœ¬åœ°è¿è¡Œ

```bash
# è¿›å…¥æ–‡æ¡£ç›®å½•
cd docs

# å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨
python -m http.server 3000

# æˆ–ä½¿ç”¨ Node.js
npx docsify serve
```

ç„¶åè®¿é—® http://localhost:3000

---

*ç”± [DeepWiki2Docsify](https://github.com/yourusername/deepwiki2docsify) å·¥å…·ç”Ÿæˆäº {time.strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        readme_file = self.output_dir / "README.md"
        readme_file.write_text(readme_content, encoding='utf-8')
    
    def _generate_index_html(self, site_name: str = "DeepWiki æ–‡æ¡£"):
        """ç”Ÿæˆ index.html"""
        html_content = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>{site_name}</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="description" content="ä» DeepWiki è½¬æ¢çš„æ–‡æ¡£ç«™ç‚¹">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <link rel="stylesheet" href="//unpkg.com/docsify@4/lib/themes/vue.css">
  <style>
    .sidebar {{
      padding-top: 6px;
    }}
    .markdown-section {{
      max-width: 800px;
    }}
    .app-name-link {{
      color: var(--theme-color, #42b983) !important;
    }}
  </style>
</head>
<body>
  <div id="app">æ­£åœ¨åŠ è½½...</div>
  <script>
    window.$docsify = {{
      name: '{site_name}',
      repo: '',
      homepage: 'README.md',
      loadSidebar: true,
      autoHeader: true,
      subMaxLevel: 3,
      maxLevel: 4,
      search: {{
        maxAge: 86400000,
        paths: 'auto',
        placeholder: 'æœç´¢æ–‡æ¡£...',
        noData: 'æ²¡æœ‰æ‰¾åˆ°ç»“æœ',
        depth: 6
      }},
      copyCode: {{
        buttonText: 'å¤åˆ¶ä»£ç ',
        errorText: 'å¤åˆ¶å¤±è´¥',
        successText: 'å·²å¤åˆ¶åˆ°å‰ªè´´æ¿'
      }},
      pagination: {{
        previousText: 'ä¸Šä¸€é¡µ',
        nextText: 'ä¸‹ä¸€é¡µ',
        crossChapter: true,
        crossChapterText: true
      }},
      mermaid: {{
        theme: 'default'
      }}
    }}
  </script>
  <!-- Docsify v4 -->
  <script src="//unpkg.com/docsify@4"></script>
  <!-- Mermaid -->
  <script src="//unpkg.com/mermaid@9/dist/mermaid.min.js"></script>
  <script>
    mermaid.initialize({{
      theme: 'default',
      startOnLoad: false
    }});
  </script>
  <script src="//unpkg.com/docsify-mermaid@1/dist/docsify-mermaid.js"></script>
  <!-- æ’ä»¶ -->
  <script src="//unpkg.com/docsify/lib/plugins/search.min.js"></script>
  <script src="//unpkg.com/docsify/lib/plugins/zoom-image.min.js"></script>
  <script src="//unpkg.com/docsify-copy-code@2"></script>
  <script src="//unpkg.com/docsify-pagination@2/dist/docsify-pagination.min.js"></script>
</body>
</html>'''
        
        index_file = self.output_dir / "index.html"
        index_file.write_text(html_content, encoding='utf-8')
        logger.info("ğŸŒ ç”Ÿæˆ index.html...")


@click.command()
@click.argument('url', required=False, default='https://deepwiki.com/Dark-Athena/PlsqlRewrite4GaussDB-web/')
@click.option('--output', '-o', default='./docs', help='è¾“å‡ºç›®å½•')
@click.option('--use-selenium/--no-selenium', default=True, help='æ˜¯å¦ä½¿ç”¨ Selenium å¤„ç†åŠ¨æ€å†…å®¹')
@click.option('--multilingual', is_flag=True, help='ç”Ÿæˆå¤šè¯­è¨€ç‰ˆæœ¬ï¼ˆä¸­è‹±æ–‡ï¼‰')
@click.option('--force', is_flag=True, help='å¼ºåˆ¶è¦†ç›–éç©ºçš„è¾“å‡ºç›®å½•ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰')
def main(url: str, output: str, use_selenium: bool, multilingual: bool, force: bool):
    """
    DeepWiki åˆ° Docsify è½¬æ¢å™¨
    
    å°† DeepWiki åœ¨çº¿é¡µé¢è½¬æ¢ä¸ºå®Œæ•´çš„ Docsify æ–‡æ¡£ç«™ç‚¹
    
    é»˜è®¤URL: https://deepwiki.com/Dark-Athena/PlsqlRewrite4GaussDB-web/
    
    ç¤ºä¾‹:
        python deepwiki_converter_fixed.py  # ä½¿ç”¨é»˜è®¤URL
        python deepwiki_converter_fixed.py https://deepwiki.com/username/project  # è‡ªå®šä¹‰URL
    """
    
    print("ğŸš€ DeepWiki åˆ° Docsify è½¬æ¢å™¨ - ä¿®å¤ç‰ˆæœ¬")
    print("=" * 50)
    
    if multilingual:
        print("ğŸŒ å¤šè¯­è¨€æ¨¡å¼ï¼šå°†ç”Ÿæˆä¸­è‹±æ–‡åŒè¯­æ–‡æ¡£")
    
    if force:
        print("âš ï¸ å¼ºåˆ¶è¦†ç›–æ¨¡å¼ï¼šå°†è¦†ç›–è¾“å‡ºç›®å½•ä¸­çš„ç°æœ‰æ–‡ä»¶")
    
    if not SELENIUM_AVAILABLE and use_selenium:
        print("âš ï¸  Selenium æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å¼")
        print("ğŸ’¡ å®‰è£… Selenium: pip install selenium webdriver-manager")
        use_selenium = False
    
    converter = DeepWikiToDocsifyConverter(url, output, use_selenium, multilingual, force)
    result = converter.convert()
    
    if result['success']:
        print("\nğŸ‰ è½¬æ¢æˆåŠŸï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {result['output_dir']}")
        print(f"ğŸ“„ å¤„ç†é¡µé¢: {result['pages_processed']} ä¸ª")
        
        print(f"\nğŸ“‹ é¡¹ç›®ä¿¡æ¯:")
        print(f"   åç§°: {result['main_page']['project_name']}")
        print(f"   æ ‡é¢˜: {result['main_page']['title']}")
        print(f"   åŸå§‹é¡µé¢: {result['main_page']['url']}")
        
        print(f"\nğŸ’¡ å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨:")
        print(f"   cd {output}")
        print("   python -m http.server 3000")
        print("   ç„¶åè®¿é—® http://localhost:3000")
        
        print(f"\nğŸŒ éƒ¨ç½²åˆ° GitHub Pages:")
        print(f"   1. å°† {output} ç›®å½•å†…å®¹æ¨é€åˆ° GitHub ä»“åº“")
        print(f"   2. åœ¨ä»“åº“è®¾ç½®ä¸­å¯ç”¨ GitHub Pages")
        print(f"   3. é€‰æ‹©ä»æ ¹ç›®å½•æˆ– docs ç›®å½•éƒ¨ç½²")
        
    else:
        print(f"âŒ è½¬æ¢å¤±è´¥: {result['error']}")
        sys.exit(1)


if __name__ == '__main__':
    main()
